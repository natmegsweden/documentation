{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Centre for Imaging Research (CIR) Documentation","text":"<p>Here is documentation regarding the core facilities under CIR at Karolinska Instiutet.</p> <p>It contains practical advice of how to work in the labs, and how to use the server and other resources.</p> <p>CIR -the Centre for Imaging Research - is a centre for world-leading imaging, jointly formed by Karolinska University Hospital, Karolinska Institutet, and Region Stockholm. CIR supports academic, clinical and industry users with access to imaging facilities and related services at the highest international level.</p> <p>For more information about CIR, please visit the CIR website.</p>"},{"location":"edit-this-wiki/","title":"How to edit this wiki","text":"<p>From the most famous wiki of them all, Wikipedia:</p> <p>A wiki is a form of hypertext publication on the internet which is collaboratively edited and managed by its audience directly through a web browser. A typical wiki contains multiple pages that can either be edited by the public or limited to use within an organization for maintaining its internal knowledge base.</p> <p>To maintain this wiki as a useful resource, each user share the responsibility of keeping it comprehensive, relevant and up to date. If you think something is missing or unclear - please make an addition or suggest a change. </p>"},{"location":"edit-this-wiki/#structure","title":"Structure","text":"<p>This wiki is really just a collection of text documents written in markdown - a very simple format for creating formatted text. You can check out this cheat sheet for some of the syntax used in markdown documents.</p> <p>These documents are tracked via the version control system git, so that changes made are reviewed, tracked and restorable. Git can run locally to track changes of projects, but it is even more useful when a project (or repository) is shared between many editors via a remote repository hosted on github.</p> <p>Each markdown document collected in the repository for this wiki represent one page of the wiki. MkDocs runs in the background to build a site from the documents in the main branch of that repository based on the settings specified in its configuration file <code>mkdocs.yml</code></p>"},{"location":"edit-this-wiki/#how-to-edit","title":"How to edit","text":"<p>You can find detailed instructions on how to edit and add pages to the wiki in the README of the repository here.</p>"},{"location":"bmic/","title":"About BMIC","text":"<p>TBA</p>"},{"location":"mrc/","title":"About MRC","text":"<p>The Magnetic Resonance Tomography Center is located in BioClinicum at Solnav\u00e4gen 30. See the KI web page for MRC for contact and booking.</p>"},{"location":"mrc/mrc-bids/","title":"BIDS at MRC","text":"<p>To be able to co-register data from different imaging modalities in CIR, share data and analysis pipelines, it is important that data collected are saved in the standard Brain Imaging Data Structure (BIDS).</p>"},{"location":"mrc/mrc-bids/#mrc-raw-data","title":"MRC raw data","text":"<p>Data collected at MRC should be pushed to FOU1/Alvik after each completed session. A unique subject-ID is created for each session and data is stored under that ID and the time of scanning, in folders numbered according to MR-sequence.</p> <p>It is therefore important to note what ID gets created and assigned at each session.</p> <p>If a subject was scanned with three sequences at 2024-12-01, 13:45:15 and given the ID: 12345, their raw DICOM data would be located on Alvik in:</p> <p><pre><code>/data/dicom/12345_20241201_134515/\n\u251c\u2500\u2500 00000001\n\u251c\u2500\u2500 00000002\n\u2514\u2500\u2500 00000003\n</code></pre> </p> <p>There are different tools available to make data collected at MRC compliant with BIDS, these are collected in the following repositories:</p>"},{"location":"mrc/mrc-bids/#mrc-dicom-to-bids","title":"MRC-DICOM-to-BIDS","text":"<p>Written to run on the MRC compute server nick-named Alvik. Gather DICOM data collected at MRC and convert to BIDS compliant naming and format. Designed to extract all sequences in the CIR-protocol for up to three sessions per subject and rename subjects to study specific ID.</p> <p>Require dcm2bids and GNU parallel in your environment.</p> <p>Note that ASL and SWI sequences are currently not defined in BIDS. See BIDS extension proposal BEP004 &amp; BEP005 for updates. For now, these are converted separately and copied to BIDS-derivatives.</p>"},{"location":"mrc/mrc-cir-data-template/","title":"The CIR data template","text":"<p>The main directory of the CIR data template contains all important directories and singularity containers for running the programs necessary for transfer, conversion to BIDS and preprocessing of data.</p> <p></p>"},{"location":"mrc/mrc-cir-data-template/#scripts","title":"Scripts","text":"<ul> <li><code>run_dicomtransfer.sh</code>: handles file transfers. The script assumes data has been organized according to the MRC standard.</li> <li><code>create_heuristic_heudiconv.sh</code>: extracts the metadata from DICOM files located in the <code>/dicom</code> directory. It generates a <code>.tsv</code> file for each DICOM file, storing the results in a hidden folder (<code>.heudiconv</code>) within the <code>/bids</code> directory. For example, metadata from a subject with ID \"1001\" will be stored at <code>/bids/.heudiconv/1001/info/dicominfo.tsv</code></li> <li><code>run_heudiconv.sh</code>: creates a basic BIDS structure from DICOM files and stores the output in the <code>/bids</code> folder. It reads the heuristics from the file <code>/heudiconv/heuristic.py</code></li> <li><code>run_fmriprep.sh</code>: Preprocesses BIDS-formatted data located in the <code>/bids</code> folder using fmriprep. The preprocessed output is stored in the <code>/bids/derivatives/fmriprep</code> directory.</li> </ul>"},{"location":"mrc/mrc-cir-protocol/","title":"3T CIR Protocol","text":""},{"location":"mrc/mrc-cir-protocol/#introduction","title":"Introduction","text":"<p>The brain imaging protocol for CIR was developed in synergy between the UK Biobank, ADNI-4  and HumanConnectom-Aging projects. CIR will establish a standard protocol at Karolinska CIR to enable multi-modality and cross-subject, cross-disease research of human brain. Hereby, we provide details of the acquisition protocols and suggestions for minimal image pre-processing.</p>"},{"location":"mrc/mrc-cir-protocol/#mri-scanner","title":"MRI scanner","text":"<p>The protocol was developed on the General Electric (GE) 3 Tesla scanner, Signa-PremierXT installed 202301, 70 cm bore, 80 mT/m max gradient strength, 200 T/m/s slew rate, and a 48 channels RF receive head-neck coil. </p>"},{"location":"mrc/mrc-cir-protocol/#mri-acquisition-protocol","title":"MRI acquisition protocol","text":"<p>The total experimental time is ca. 34 min. The protocol contains the following 7 MR contrasts (details below) * T1-weighted (4:46 min) * T2-weighted FLAIR (4:15 min) * Scout (0:09 min) * Arterial Spin Labeling (5:02 min) * FieldMap fmri (0:05 min) * Functional MRI (Resting-state, eyes open, 6:02 min) * FieldMap DTI (0:39 min) * Diffusion Tensor Imaging (8:27 min) * Susceptibility-weighted imaging (1:41 min) * T2-weighted structural imaging (2:26 min)</p>"},{"location":"mrc/mrc-cir-protocol/#t1-weighted-446-min","title":"T1-weighted (4:46 min)","text":"<p>3D structural imaging provides T1- relaxation time contrast between grey and white matter while CSF is nulled. It is used for (i) medical assessment, (ii) inter-subject, cross-modality alignments, and (iii) brain segmentation.</p> <p>3D MPRAGE Sagittal, resolution 1 mm3, 180 slices, TR=2.237, FA=9, BW=244.1 Hz/pix, Parallel Imaging: ARC R=2, Coil Intensity Corrections and Gradwarp applied. Prospective motion correction - PROMO\u2122 is used.</p>"},{"location":"mrc/mrc-cir-protocol/#t2-weighted-flair-415-min","title":"T2-weighted FLAIR (4:15 min)","text":"<p>3D structural imaging provides a transverse-relaxation time contrast between GM, WM, while CSF is nulled. It is used for (i) medical assessment (ii) inter-subject, cross-modality alignments, and (iii) brain segmentation.</p> <p>3D IR - Fast Spin Echo (CUBE), sagittal, resolution 1 mm3, 180 slices, TE=130 ms, TI=1894 ms, Tr=6800, Parallel Imaging: ARC R=2x2, CS  R=1.2, B1, Coil Intensity Corrections and Gradwarp applied. </p>"},{"location":"mrc/mrc-cir-protocol/#scout-009-min","title":"Scout (0:09 min)","text":"<p>Refine head position, patient may slide down while the following imaging requires accurate slice positioning.</p>"},{"location":"mrc/mrc-cir-protocol/#arterial-spin-labeling-502-min","title":"Arterial Spin Labeling (5:02 min)","text":"<p>3D quantitative imaging of blood perfusion weighted contrast that uses arterial blood water as an endogenous tracer to measure blood flow through brain parenchyma.</p> <p>3D Fast Spin Echo-Spiral, axial, eff.resolution 3.5 mm3, 46 slices 3.5 mm thick, Labeling duration 1450 ms, Post-labeling delay 2525 ms, vascular crushers: none, Gradwarp applied. Dataset contains: M0, PW (difference image tag-control) images zero-filled to 128x128 matrix. </p>"},{"location":"mrc/mrc-cir-protocol/#fieldmap-fmri-005-min","title":"FieldMap fmri (0:05 min)","text":"<p>The same setting as for FMRI but reverse phase view order. </p> <p>Type-in psd: epiRTrs, CV blips =1, distortion A&gt;P, phase enc. dir. \u201cj-\u201d</p>"},{"location":"mrc/mrc-cir-protocol/#functional-mri-resting-state-eyes-open-602-min","title":"Functional MRI (Resting-state, eyes open, 6:02 min)","text":"<p>Time series data provides contrast sensitive to blood oxygenation associated with intrinsic brain activity of GM. It is used for functional connectivity mapping as well as other derivative metrics of tissue oxygenation are produced.</p> <p>2D Gradient Echo EPI, axial (no tilt), resolution 3 mm3, 44 slices, 3 mm thick,  TR=720 ms, FA=60\u00b0, TE=28 ms, Parallel Imaging: ARC R=1, MultiBand R=4 (11 bands), Eff.echo.spacing= 516\u00b5s, Tot.readout=0.037668 s, Fat Sat \u2013 classic. Dataset: number of volumes=500, num.dummy scans =3 excluded from dataset, image ACQ matrix: 74x74 , Recon matrix: 128x128. (type-in psd: epiRTrs, CV blips =0, distortion P&gt;A, phase enc. dir. \u201cj\u201d  ).</p>"},{"location":"mrc/mrc-cir-protocol/#fieldmap-dti-039-min","title":"FieldMap DTI  (0:39 min)","text":"<p>The same setting as for DTI but reverse phase view order (blips polarity) Type-in psd: epi2as, distortion A&gt;P, phase enc. dir. \u201cj-\u201d</p>"},{"location":"mrc/mrc-cir-protocol/#diffusion-tensor-imaging-827-min","title":"Diffusion Tensor Imaging (8:27 min)","text":"<p>Provides contrast based on diffusion properties of intra and extra cellular water. Local diffusivity metrics are sensitive to the integrity of tissue compartments (multiple b-values required). Anisotropic diffusion probed along multiple directions is sensitive to structural connectivity between brain regions.</p> <p>2D Spin Echo EPI, axial (no tilt), resolution 2 mm3, 72 slices 2 mm thick, two diffusion shells b1=1300 s/mm2 50 dir\u2019s and b2=2600 s/mm2 50 dir\u2019s, # b0=6, TE=75 ms, gradient directions adopted from UK Biobank project,  Parallel Imaging: ARC R=2, MultiBand R=2,  Eff.echo.spacing =337\u00b5s, Totl.readout=0.03634s,  FatSat: classic and Slice-Selective  Gradient Reversal,  image ACQ matrix = Recon matrix= 110x110  (type-in psd: epi2asaltoff, distortion P&gt;A, phase enc. dir. \u201cj\u201d  ).</p>"},{"location":"mrc/mrc-cir-protocol/#susceptibility-weighted-imaging-141-min","title":"Susceptibility-weighted imaging (1:41 min)","text":"<p>Provides contrast sensitive to magnetic susceptibility differences between tissue constituents (e.g., iron, calcium and myelin). It is used for (i) medical assessment (ii) for characterization of iron, myelin, micro-bleeds, etc.</p> <p>3D EPI Gradient Echo, axial, resolution 0.5x0.5x2 mm, single echo, TE=27 ms, TR=56 ms, FA=20\u00b0, number of multi-shots 20, matrix 448x448, 80 slices 2 mm thick, Fat Sat: SpSp rf-pulse. Two datasets is produced: SWI and local tissue phase map. Raw data is not returned, processed data is returned with some delay.</p>"},{"location":"mrc/mrc-cir-protocol/#t2-weighted-structural-imaging-226-min","title":"T2-weighted structural imaging (2:26 min)","text":"<p>Provides a T2-relaxation time contrast between GM, WM, while CSF is not nulled. It is used for (i) medical assessment (ii) tissue segmentation.</p> <p>3D Fast Spin Echo (CUBE\u2122), sagittal, resolution 1 mm3, 180 slices, TE=90 ms,  Parallel Imaging: ARC R=2x2, CS R=1.2, B1, Coil Intensity Corrections and Gradwarp. </p>"},{"location":"mrc/response-equipment/mrc-eyetrack/","title":"Eye-tracker","text":"<p>MRC provide the EyeLink 1000 Plus eyetracker from SR Research. This is the same eyetracker available at NatMEG. The eyetracker is mounted on a \"long range mount\" to record from the distance required during scanning.</p> <p>Eyetracking require a front-coated mirror mounted to the head coil. Other mirrors does not reflect enough IR-light for the eyetracker to pick up the corneal reflex reliably.</p> <p></p> <p>See this page from SR research for installation guide and user manual</p>"},{"location":"mrc/servers/","title":"MRC servers","text":""},{"location":"mrc/servers/#server-architecture","title":"Server architecture","text":"<p>Karolinska Institute's MR centrum owns five imaging servers: * Alvik: Compute server running Linux CentOS 7, with access to the storage server (where your data is located). * Slussen: Storage server * Skanstull: Storage server (clone) * Fou1: DICOM server, not in use for the moment (August 2020). * Fou2: DICOM server (in use, supposed to be a clone of fou1)</p>"},{"location":"mrc/servers/#data-organization","title":"Data organization","text":"<ul> <li>DICOM files are stored under the directory /data/dicom</li> <li>Each study is stored under a specific directory whose name is a concatenation of the examination number, the date, and the time. \\   E.g. study 38456 performed on July1st 2022 at 13:01:20 will be stored under: /data/dicom/38456_20220701_130120/</li> <li>Inside that directory, one can find all the sequences that have been run. </li> <li>Most often directory 00000001 contains the localizer, 00000002 a calibration. The rest of the directories, 00000003, 00000004 etc, can include structural T1, fMRI, DTI images. fMRI and DTI sequences generate several thousand DICOM files inside a directory.</li> <li>Every morning at 1:00 a.m., any DICOMs that have not yet been transferred from fou2 are automatically copied.</li> </ul>"},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/","title":"Connect to Alvik","text":"<p>This is a guide on how to connect to MR Center's computational server for data analysis. To connect, you need a username and password for the server.</p> <p>As a user you can only connect to Alvik and your home directory will be <code>/data/myuser</code>.</p> <p>To connect to the server, you must be on the KI network either via a wired connection or by using FortiClient.</p>"},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/#connect-to-alvik-with-turbovnc","title":"Connect to Alvik with TurboVNC","text":"<p>Running VNC has the advantage of giving you the whole desktop of Alvik and to allow you to shut down your laptop or desktop while keeping your batch running on the server. 1. Download and install the software here: TurboVNC 2.  Make a new TurboVNC connection. Write your username followed by @193.10.16.82:</p> <p></p> <ol> <li>Click \"Connect,\" and you will be prompted to enter your password.</li> </ol>"},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/#setup-turbovnc-on-windows","title":"Setup TurboVNC on Windows ?","text":""},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/#connect-to-alvik-via-terminal","title":"Connect to Alvik via Terminal","text":"<p>You can log in to Alvik via terminal with the following command:</p> <p><code>ssh</code><code>&lt;username&gt;</code><code>@193.10.16.82</code></p>"},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/#data-transfer","title":"Data transfer","text":"<p>The transfer protocol for copying/reading files to/from Alvik is sftp. Simple ftp is not allowed. </p> <p>This is a guide on how to transfer data from or to Alvik using the terminal or via Filezilla</p>"},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/#using-filezilla","title":"Using Filezilla","text":"<ol> <li>Download and install the FileZilla client for your OS.</li> <li>Connect to the server with the following settings: \\    Hostname: <code>193.10.16.82</code>, port: <code>22</code>, your username and password. \\    Once connected, you can upload or download files between Alvik and your local machine.</li> </ol>"},{"location":"mrc/servers/set-up-connection/01_Connect-to-Alvik/#using-the-terminal","title":"Using the terminal","text":"<ol> <li>Connect to the server with sftp:</li> </ol> <p> <code>sftp _username_@193.10.16.82</code> and enter your password when prompted.</p> <ol> <li>To download data from Alvik to your computer, you can use the following command:</li> </ol> <p><code>get -r &lt;remote_directory&gt; &lt;local_directory&gt;</code></p> <ol> <li>To upload data to Alvik from your computer, you can use the following command:</li> </ol> <p><code>put -r &lt;local_directory&gt; &lt;remote_directory&gt;</code></p>"},{"location":"natmeg/","title":"About NatMEG","text":"<p>NatMEG is the first and only research laboratory in Sweden for whole-head measurements of neuronal brain activity using magnetoencephalography (MEG). The NatMEG lab was  inaugurated in October 2013, and is one out of only approximately 200 active MEG labs worldwide.</p> <p>Since 2024, we have the worlds largest on-scalp MEG system that uses optically pumped magnetometer (OPM) sensors, a new type of quantum enhanced sensors that can measure closer to the scalp than conventional MEG sensors.</p> <p>For contact details, booking, etc. see natmeg.se where you also find a video introduction to MEG, and lectures from leading researchers on theory and concepts and data analysis.</p>"},{"location":"natmeg/#analysis-software","title":"Analysis software","text":"<p>Most scripts used at NatMEG rely on one of two well-documented and free software tools available for analysis of MEG and EEG data:</p> <ul> <li>MNE - Python package</li> <li>Fieldtrip - Matlab toolbox</li> </ul>"},{"location":"natmeg/cerberos/","title":"Cerberos","text":"<p>TBA</p>","tags":["NatMEG","analysis"]},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/","title":"OPM MANUAL","text":""},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#introduction","title":"Introduction","text":"<p>This document explains the main steps you need to follow to run the Hedscan software.</p> <p>Some utility scripts that will be mentioned in this document can be find the path \"~/opm-utility-scripts/\". </p> <p>If one wants to do an OPM recording in parallel with some recording with the MEG scan, it is important that when defining the preparation, the \"internal active shielding\" is off as internal active shielding interferes with the OPM system. To check so, click to \"change\" in Acquisition and check that the bottom checkbox called \"internal active shielding\" is off.</p> <p> Figure 1: left: Free-standing helmet mount. right: sensors pulled out to be flush with inner helmet surface.</p> <p>Before starting make sure the OPM-MEG helmet is mounted and fully secured to the free-standing or chair mounted support and that all sensors are pushed out so that they are flush with the inside of the helmet. If you need to mount the helmet make sure to always be at least two people, with one taking care of the cables to avoid tension. Ask for help from an experienced OPM user if you have not been trained in how to mount the helmet. </p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#switching-on-the-system","title":"Switching on the system","text":"<p>To turn on the system, you need to go behind the MEG room and find the Hedscan system.</p> <p>In the figure the buttons that need to be turned on can be seen. However, it is important to follow the correct order.</p> <p> Figure 2: Hedscan system</p> <p>First, you needs to turn on the power button on the bottom left corner. Then, turn on the individual rack buttons (the ones on the right) one by one from top to bottom with ~3 seconds pause in between. </p> <p>After that, you need to go back to the Hedscan host computer and open the Hedscan software (which you can find in the desktop). Once the software is open, it will detect that there is a HEDscan system that is on. If not, one may need to wait a few moments. Click on it and click to connect (see following figure). </p> <p> Figure 3: Screenshot of Hedscan UI when connecting with a Hedscan system</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#starting-up-the-sensors","title":"Starting up the sensors","text":""},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#overview","title":"Overview","text":"<p>To use Hedscan with your participant you will need to correctly position the helmet on the person's head. In this helmet you will find different helmet slots (like holes) where the OPM sensors can be placed. The helmet slots are named as L/Ryyy where yyy is a number describing the location on the helmet. Then, sensors can be placed inside a helmet slot and wired to a specific electronic channel named sXX where XX is a number describing the location of the channel in the rack.</p> <p>When the software is connected, you will see all the channels that have a sensor connected to it in the different sensor cards. Other important cards we may want to pay attention to are \"digital in\" and \"HPI\". Make sure to enable the HPI and digital input channels by double-clicking on the respective symbols of: hpi217, 218, 219 and 220 and digital in (they turn blue when enabled).</p> <p> Figure 4: Screenshot of the UI when you first opens the software</p> <p>The \"digital in\" receives triggers from the stimulation computer and can be enabled by double clicking on it. The \"HPI\" card shows the HPI channels and they can be turned on to enable the each channel by double clicking on them too. </p> <p>Regarding the buttons, there are 8 buttons in you interface that are very important and useful and can be subdivided into 3 groups:</p> <ol> <li>The three buttons in the top left, which are the sequence for starting the system up (see number 1 in Figure 3):</li> <li>\"Restart sensors\": button that restarts the sensors wired to those channels that are selected</li> <li>\"Initialize system\": button that initializes the sensors wired to those channels that are selected</li> <li> <p>\"Localize sensors\": button that localizes the sensors wired to those channels that are selected.</p> </li> <li> <p>The two buttons in the middle, which are for selecting channels and turning them off  (see number 2 in Figure 3):</p> </li> <li>\"sensor select\": button used to select the channels based on the state or selecting all of them. The \"select all\" option selects all the channels and the \"deselect all\" deselects all the selected ones. Then, there are different options for selecting the channels based on the state, for example \"select orange\" for selecting those channels that have an OPM sensor connected but not ready, the \"select green\" for selecting the channel with restarted sensors (sensor ready but not initialized), the \"select blue\" for selecting the initialized ones and the \"select red\" for selecting the error ones.</li> <li> <p>\"sensors off\": button to turn off the sensors that are selected</p> </li> <li> <p>The three top right buttons, which are to open other windows  (see number 3 in Figure 3):</p> </li> <li>\"Data Vis\": button to open the data visualization screen where the data of all the enabled channels is shown</li> <li>\"Recorder\" button to open the recorder screen to start a recording</li> <li>\"settings\" button to change the settings of the software.</li> </ol> <p>Also, in the bottom left corner (see number 4 in Figure 3) you can find the button to open the command window. </p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#restarting-sensors","title":"Restarting sensors","text":"<p>When you first open the software, all the channels will be in orange color, which means that an OPM sensor is connected to that channel but not ready. If a channel is gray it means there is no sensor connected to the channel. You now need to go to \"sensor select\" to select all the channels and restart the sensors wired to those channels by clicking on \"restart sensors\". The channels will start blinking until they stop and change to green (may taKe a few minutes). If a channel turns red, it means that there was an error. You can go to sensor select and select all the red channels, or just directly click on the red channel you want to restart and click to restart. If the channel persistently turns red, it may be preferable to turn it off. To do so, select the red channels and press \"Sensors off\". </p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#initializing-sensors","title":"Initializing sensors","text":"<p>Once all the channels are green (which means that there is a wired sensor ready but not initialized) you can click on \"sensor select\" to select the green channels and then click on initialize to initialize the sensors wired to those channels. </p> <p>It is important to mention that this process needs to be done with the MSR door closed.</p> <p>Again, the channels will start blinking until they stop and turn blue. If some channel still remains green, you can select it and initialize it again.  If the problem persists, you may turn off the channel in question so it will not interfere with its neighbors.</p> <p> Figure 5: Screenshot of the UI when the sensors are green and some of them red.</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#localizing-the-sensors","title":"Localizing the sensors","text":"<p>Once all the channels are blue you can select them and localize the sensors wired in those channels by clicking on \"localize sensors\". This will determine which sensor is in which helmet slot (physical location) as well as at which depth and orientation. Again, the channels will start blinking until they stop. You can check if all the sensors were localized if in the \"localize sensors\" button it appears (128/128) which means that the 128 sensors were localized. This number can vary depending on which sensors are initialized. </p> <p>Once the sensors are localized, in the ~/.local/share/HEDscan/helmetscan/ path a CSV is created with the sensor's location and orientation. In the utility scripts folder, there is a script called \"plot-helmetscan-3D-vtk\" that can be used to check in a 3D view layout which helmet slots have a sensor connected to which channel and which helmet slots are empty. To do so, when you run the code, it will ask you to select the CSV file with the sensors localizations. The channels that have a sensor connected and localized, will appear as white dots with the corresponding channel name, and those slots that are empty (there is no sensor in that helmet slot connected to a channel), will appear as read dots with the corresponding name of the helmet slot. This script allows one to detect if sensors in a region critical for the experiment are not working and replace it (they need to restart everything if they do so).</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#empty-room-recording","title":"Empty room recording","text":"<p>Once the sensors are localized you can click on \"recorder\". On \"Folder\"  (see number 1 in Figure 5) you can see the main directory where it will be saved. Normally, this directory does not need to be changed. Inside that directory, there should be the project folder and the name of that folder should be written on \"Project name\"  (see number 2 in the following Figure). If there is an existing project folder you should be careful to spell it correctly or a new folder will be created. Then the \"operator\" (which is optional) and the \"Subject ID\" (see numbers 3 and 4 in the following Figure). Also, you need to write the \"File\" which normally corresponds to the name of the paradigm, for example, \"EmptyRoom\" (see number 5 in the Figure). </p> <p>Once all of these names are provided, you can click on \"Record\". If you know the exact recording time, you can turn on the \"timed recording\" and write the number of seconds before clicking on recording. The recording will then automatically stop once the timer runs out. Otherwise, you can stop the recording manually as needed. When clicking stop the file with the recorded data will be saved automatically. </p> <p>Probably, all the different recordings that you may want to record in a single session are related to the same project and participant. Thus, you can have the recording window open and just change the file name between recordings. </p> <p> Figure 6: Screenshot of the recording window</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#participant-going-inside-the-room","title":"Participant going inside the room","text":"<p>In addition to the regular \"conventional MEG preparation\", make sure to always put an EEG cap. This is to provide additional thermal insulation (in participants with little hair) and avoid restricting air flow (in participants with a lot of hair). Once all the previous steps are done and the participant is prepared, the participant can already go inside the shielded room. Make sure to turn off the sensors before opening the door. </p> <p>Place the participant in the chair and connect all the necessary cables such as the HPI coils cable (which goes into the HEDscan connector, not the TRIUX), the ECG/EOG electrodes and (if applicable) the EEG cap. Put in ear-tubes if required by for the experiment and - if running EEG - add a hair net to avoid leaving EEG gel on the sensors. Then carefully lift the participant up into the helmet until they start feeling the top of the helmet. After that, the sensors can be pushed in, being careful about not applying too much pressure to the participant. To avoid unintentionally pushing the participants head to one side we advise to center their head at the start by pushing in a few sensors on both sides. </p> <p>Be more careful with those sensors that can directly touch the skin (mainly in the ears). If that is the case, push the sensor a little out to avoid direct contact with the skin. </p> <p>Once all sensors are in place, give to the participant the alarm bubble, and other stimulus/response equipment needed for the paradigm. Then, close the door and repeat the same procedure described in section 3. </p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#hpi-pre","title":"HPI pre","text":"<p>Once the empty room is recorded, an HPI recoding needs to be carried out before the main recording. This is necessary since OPMs do not have as high bandwidth and thus, the HPI coils cannot be recorded at the same time as measuring brain activity.</p> <p>To run the HPI coils, there is a .txt file in the desktop called \"How to run HPI\" with the steps that you needs to follow. First, start the recording, and then copy the terminal command line form the text file, paste it in the terminal console and press enter. The terminal console command can be found in the down left corner in the main window. </p> <p>The script will sequentially activate the HPI coils and stop the recording automatically. </p> <p>After that, one can also run the check-hpi.py script to make sure that the recording was good. To do so, once the .fif file was created in the corresponding folder when running the \"HPIpre\", one can run check-hpi.py in the terminal and select the corresponding .fif file. If you see bad fits, check if the hpi coils are properly connected or if they may have moved.</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#recording","title":"Recording","text":"<p>Make sure to check free disk space before starting a recording: The Hedscan software does not raise an error if it runs out of storage space. It will appear to continue recording but no more data is saved and resulting in a corrupted file. As a rough rule of thumb, assume you need around 2 GB for every 10 minutes of recording. If there is not enough space, remove older recordings but make sure to back them up by copying to <code>/data/temp/</code> on cerberos before deleting if unsure or if they belong to another project.</p> <p>When all of the previous steps are done, you can actually start with the main recording and stop it whenever necessary. The data can be visualized in \"Data Vis\". </p> <p>When starting the recording check that triggers show up as expected. The triggers should be seen in \"Data Vis\", in a channel called di38. Throughout the recording monitor the participant.</p> <p>Unlike TRIUX system, Hedscan system saves longer recordings in a single file (it does not split them into 2 GB files). Since this can create problems for some storage filesystems, we recommend limiting file sizes by adding breaks in the paradigm every 10-15 minutes during which a recordings can be stopped and new ones started. If recording HEDSCAN and TRIUX simultaneously, we recommend starting/stopping both systems to make stitching the data together afterwards simpler.</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#data-visualization","title":"Data Visualization","text":"<p>\"Data Vis\" can be used to display the data of those sensors that are correctly initialized and localized. Each row corresponds to the data of a sensor. When doing a recording of an experiment that sends triggers to the OPM recording system, you need to make sure that you see those triggers that you expect in the data as vertical lines. </p> <p>On the top left part of the visualizer window you can find different icons. Some of the most relevant ones are mentioned below (see the image below).</p> <p> Figure 7: Data visualization buttons.</p> <p>The second icon is used to apply some digital filters to the data. These filters are only for visualization purposes since those are not applied to the recorded data. This can help you to identify some specific patterns in your data. You need to select a filter (low pass, high pass...) and then you can choose the channel/s you will apply it to, the cut off frequency, the type and order (see the following figure). </p> <p> Figure 8: Filter selection.</p> <p>The third icon can be used to open the online-FFT visualizer of your data. Choose averaging of 10 spectra and a frequency resolution of ~1Hz and press the play button. If you want to change the number of averages or frequency resolution, stop the FFT (pause button), change the parameters and press play again.</p> <p>The fourth one, can be used to change the scaling of the visualization of the data. You can increase or decrease the space between each row in graph spacing. The time discretization of the visualization can be changed \"time\" and the scaling of the amplitude of the data can be changed in \"major grid scaling\". </p> <p> Figure 9: Scaling settings</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#hpi-post-optional","title":"HPI post (optional)","text":"<p>After all the recordings related to a participant, you can optionally run the post HPI. To do so, the same steps explained in the HPI pre section, need to be followed for the HPI post. </p> <p>Moreover, the add-hpi script (or add-hpi-multi if there are multiple recordings) can be run in order to add the device-to-head transformation that is needed for MNE python to the fif-file.</p>"},{"location":"natmeg/opm-acquisition/01_OPM-Acquisition/#after-the-recording","title":"After the recording","text":"<p>After finishing the recording, turn off the sensors and enter the MSR to let the participant out. Pull out the sensors until the participant can pull their head out of the helmet and only then carefully lower them. Disconnect all the cables and let the participant out of the MSR for debriefing and changing back to their clothes. In the MSR, depending on whether someone will record OPM-MEG right after you, push all the sensors out to be flush with the inside of the helmet (if someone is recording OPM-MEG after you) or push them fully in for putting the system in the storage closet (if someone is recording SQUID-MEG after you). Only move the helmet together with a second person that takes care of the cables and ask for help from an experienced OPM-MEG user if you have not been trained in it.</p> <p>Unless someone is recording right after you, make sure to turn off the OPM rack in reverse order from \"Switching on the system\".</p>"},{"location":"natmeg/opm-acquisition/02_OPM-specs/","title":"OPM specifications","text":"<p>On-scalp magnetoencephalography (MEG) The new OPM-MEG system at NatMEG is a Fieldline HEDSCAN with 128 OPM sensors, active shielding panel and smart helmet.</p> <p></p>"},{"location":"natmeg/other/Acoustic-panels-specs/","title":"Acoustic Panels","text":""},{"location":"natmeg/other/Acoustic-panels-specs/#takustik-pet-ceiling-absorber-120-wh","title":"t.akustik PET Ceiling Absorber 120 WH","text":"<p>Manufacturers specification: Absorption NRC = 0.95 Density: approx. 50 kg/m3 Material: PET fleece Dimensions (H x W x D): 1200 x 600 x 50 mm Weight: 2.5 kg Flame retardant according to the ASTM E84 - Class 1  </p> <p>8pcs mounted with brass eyelets, nylon string and 3D-printed PLA brackets.</p> <p></p>"},{"location":"natmeg/other/Acoustic-panels-specs/#clearsonic-s2466x2-sorber","title":"Clearsonic S2466x2 Sorber","text":"<p>Dimension: 1670 x 609 x 38 mm  </p> <p></p>"},{"location":"natmeg/other/Beyond-compare/","title":"BeyondCompare","text":"<p>BeyondCompare is a file-manager and ftp-client primarily used to upload your files to the Archive server.</p>"},{"location":"natmeg/other/Beyond-compare/#before-measurement","title":"Before measurement","text":"<p>No action requiered</p>"},{"location":"natmeg/other/Beyond-compare/#during-measurement","title":"During measurement","text":"<p>No action requiered</p>"},{"location":"natmeg/other/Beyond-compare/#after-measurement","title":"After measurement","text":"<ol> <li>To view your data path on Archive type:</li> </ol> <p>your_project_name@archive.natmeg.se log in using your credentials</p> <ol> <li>Make sure you have the local data path corresponding to the server path</li> <li>Sync or copy files between DANA and Archive</li> </ol> <p>When you have finished processing your files on DANA (eg. MaxFilter), and uploaded everything to Archive, please delete the files from DANA in order to save space</p>"},{"location":"natmeg/other/File-naming/","title":"File naming","text":"<p>Plan in advance how you want to name your files. You may have different conditions or times of measurement. Plan also for how you want to name the files if you have to stop recordings.</p> <p>! Acquisition does not allow special characters or spaces in the filename</p>"},{"location":"natmeg/other/File-naming/#issues","title":"Issues","text":""},{"location":"natmeg/other/File-naming/#fixing-wrong-filenames-of-recordings","title":"Fixing wrong filenames of recordings","text":"<p>Problem: One or more recording is saved with a wrong filename</p> <p>Solution: rename filenames (three ways)</p> <ol> <li>Open a terminal <ol> <li>cd to data folder (replace text with the text that applies your project): <p>/neuro/data/sinhue/your_project_name/NatMEG_number/YYMMDD</p> </li> <li>Rename the file: <p>mv old_filename.fif new_filename.fif</p> </li> <li>Press enter.</li> </ol> </li> </ol> <p>! Be aware that if a file with the new filename already exists, it will be overwritten with no option to recover the lost data. Rename any overlapping named file first.</p> <ol> <li> <p>Open folder window</p> <ol> <li>Go to /neuro/data/sinhue/your_project_name</li> <li>Right click and rename file</li> </ol> </li> <li> <p>Open BeyondCompare</p> <ol> <li>Open your project path</li> <li>Right click and rename file ! Using BeyondCompare on DANA you can also rename files you have uploaded to Archive</li> </ol> </li> </ol>"},{"location":"natmeg/other/MSR-specs/","title":"Magnetically Shielded Room (MSR)","text":""},{"location":"natmeg/other/MSR-specs/#magnetically-shielded-room-msr","title":"Magnetically Shielded Room (MSR)","text":"<p>All MEG/EEG recordings are done within a two-layer MSR, model Ak3B from Vacuumschmelze GmbH.</p> <ul> <li>Further technical specifications for the MSR are found here.</li> <li>For an overview of  the MSR shielding factor across frequencies, see here.</li> </ul> <p>Since October 2022 the MSR is equipped with Acoustic panels.</p> <p>Luminance (date of measure): TBA</p>"},{"location":"natmeg/other/Maxfilter/","title":"Maxfilter","text":"<p>It is convenient to use a script to loop through your raw fif-files with MaxFilter. MaxFilter is installed on DANA.</p>"},{"location":"natmeg/other/Maxfilter/#before-measurement","title":"Before measurement","text":"<ol> <li>Copy the master /home/master/data_scripts/avg_headpos/maxfilter_avgHead.sh to your own directory.</li> <li>Change the settings in the headers to match your desired processing pipeline.</li> <li>Make executable</li> </ol> <p><code>chmod u+x &lt;your_file_name.sh&gt;</code></p> <p>! To check if the file is executable type <code>ls -l &lt;your_file_name.sh</code></p>"},{"location":"natmeg/other/Maxfilter/#during-measurement","title":"During measurement","text":"<p>No action required</p>"},{"location":"natmeg/other/Maxfilter/#after-measurement","title":"After measurement","text":"<ol> <li>Run your personalized maxfilter script (cd into correct folder)</li> </ol> <p><code>./&lt;your_file_name.sh&gt;</code></p> <ol> <li>When analysis is done, upload to server</li> </ol>"},{"location":"natmeg/other/Maxfilter/#maxfilter-names-guide","title":"Maxfilter names guide","text":"<ul> <li>sss: processed with Signal Space Separation.</li> <li>tsss: processed with Temporal Signal Space Separation.</li> <li>mc: applied movement correction.</li> <li>ds: data is downsampled.</li> <li>quat: quaternion; it has estimated the head movement but not done movement correction (this is represented in quaternion format).</li> <li>avghead: TBA</li> </ul>"},{"location":"natmeg/other/Maxfilter/#note","title":"Note","text":"<p>This pipeline is a wrapper for running Neuromag MaxFilter inside the NatMEG infrastructure at Karolinska Insitutet, Sweden (www.natmeg.se). Neuromag MaxFilter is a commercial software licenses by Electra Neuromag. The head position averagers are written in Python and use functions from MNE-Python (https://martinos.org/mne/stable/index.html). The pipeline has been tested to work on DANA, but no guarrantee is provided that it will work elsewhere!</p>"},{"location":"natmeg/other/Monitors/","title":"Monitors","text":""},{"location":"natmeg/other/Monitors/#dual-monitors","title":"Dual monitors","text":"<p>You can use dual monitors on the Stimulation PC if you want to run Presentation separate from the screen shown to the participant in the MSR, for example, if you are going to show a video to the participant instead of the Presentation screen.</p> <ol> <li>On the Stimulation PC, right-click on the Windows desktop and select Screen Settings.</li> <li>Change the so that the screens are extended. Confirm changes.</li> <li>On the second screen (the one connected to the eye tracker), switch input to DVI.</li> </ol> <p>Presentation will still as a default run on the first monitor. Change the monitor by changing your Presentation project settings. Under the menu \"Monitors\" you change the driver from \"Standard Driver\" to the one with a number.</p>"},{"location":"natmeg/other/Monitors/#after-measurement","title":"After measurement","text":"<p>Remember to switch the screens back to the standard setting when you are done.</p>"},{"location":"natmeg/other/Triggers-specs/","title":"Triggers and time-locking","text":"<p>A total of 32 digital channels and 24 analog channels  are sampled in parallel and synchronized to the MEG,  EEG and biochannels data.</p> <ul> <li>Further technical specifications are found here.</li> <li>System technical manual is found here.</li> </ul>"},{"location":"natmeg/preparation/01_Electrodes-standard-setup/","title":"Electrodes (standard setup)","text":"<p>This is the first part of the preparation which is followed by the Digitization</p> <p>The standard setup of electrodes includes: - 2 horizontal EOG - 2 vertical EOG - 2 ECG - 1 Reference - 1 Ground</p>"},{"location":"natmeg/preparation/01_Electrodes-standard-setup/#material","title":"Material","text":""},{"location":"natmeg/preparation/01_Electrodes-standard-setup/#step-by-step-guide","title":"Step-by-step guide","text":"<ol> <li>Place the participant in the wooden chair</li> <li>Use abrasive gel to scrub where Electrodes will be placed. Usually a pie-sized portion on a cotton pad is enough for all electrodes, and 3-5 circular wipes will do. <p>! Be careful and check with the participant so it does not hurt</p> </li> <li>Wipe with alcohol swaps to remove gel. Also wipe positions of the HPI coils. <p>! Warn participant that if might sting a bit</p> </li> <li>After letting the alcohol dry a few seconds attach the electrodes at the location indicated by the picture. <p>! You can cut away part of the electrode (not too much!) to avoid participant getting it in the eye.</p> </li> </ol> <p></p>"},{"location":"natmeg/preparation/01_Electrodes-standard-setup/#check-impedance","title":"Check impedance","text":"<p>When all electrodes are attached. Check impedance using the impedance meter.</p> <ol> <li>Connect the three cables to the electrodes according to their labels. </li> <li>Click the middle white button to turn SIGGI on and select Impedance Meter. Press white button again.</li> <li>Read impedance for each channel, by moving the connector to the different electrodes. Impedance levels should not be higher than 20 ${k\\Omega}$, but the less the better. If impedance is too high, try to press on the electrode. If the signal is still not good, remove the electrode, clean the skin and attach a new.</li> </ol> <p></p> <p>When finished with the electrodes move on to the Digitization.</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/","title":"Digitization","text":"<p>The digitization is the second part of the preparation. The first part is the placement of electrodes.</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#why","title":"Why","text":"<p>Head positioning should be monitored either continuously throughout the acquisition or at the start and end of the recording. The MEG acquisition is done only with respect to the MEG device, instead of the anatomy of the subject. Therefore, MEG devices include a subsystem to determine the position of the head with respect to the MEG sensors. As MEG (unlike MRI) cannot directly measure the position of the head, small coils known as Head Position Indicator coils (HPI) placed at known locations on the scalp of the subject, when energized, will generate a magnetic field that helps us to localize the position of head in a three-dimensional space, with respect to the MEG sensor array. If continuous head position tracking is enabled, generally small movements are acceptable with a maximum error of 5 mm.</p> <p>Information about the patient's head position, orientation, and shape is obtained by digitizing (3D digitizer) the standard fiducial points, HPI coils, and the required additional points creating Cartesian co-ordinates in a 3D space. Digitization of four HPI coils, and landmarks, which include three bony fiducial points (Nasion, left, and right pre-auricular points), and additional points, is performed.</p> <p>The HPI coil positions, and hence the head position, are estimated from the coil signals. This estimation is done several times per second, allowing the system to track also relatively fast movements. Once the head position is estimated, the MEG signals are transformed to a reference head position. This conversion is sequentially performed at each time point throughout the continuous (raw) data file.</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#equipment","title":"Equipment","text":"<p>TBA</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#before-measurement","title":"Before measurement","text":"<ol> <li>Check so that the correct HPI preparation is loaded</li> <li>Remember to check  cHPI  in the Acquisition window</li> </ol> <p>To monitor head position, open a terminal and type:  <pre><code>mneHeadPos\n</code></pre></p> <p></p> <p>The head position is read from the latest HPI fit. Click\" Reload HPI\" to update the view.</p> <p>! Be aware that the head position tool does not show the actual participants head. The head is a template head and should only be used as an approximation of the participant's real head position in the MEG helmet.</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#during-measurement","title":"During measurement","text":"<ol> <li>After each time you've stopped a measurement you need to check the  cHPI box again</li> </ol> <p>You can rotate the head in the head position tool using the buttons on the GUI or by using the mouse wheel. You can change what is displayed (helmet, transparency, HPI on/off, head point fits, etc.) by pressing \"Options...\".</p> <p>To view the head position from a previous file (e.g. for comparison) open a new terminal window and type:</p> <p>/data/MNE/mne_visualize_hpi_file your_file_name</p> <p>The your_file_name is the filename of the file you want to read including the full path</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#after-measurement","title":"After measurement","text":"<p>Close head position tool</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#issues","title":"Issues","text":""},{"location":"natmeg/preparation/02_Digitization-hpi/#acquisition-does-not-ask-about-measuring-hpi","title":"Acquisition does not ask about measuring HPI","text":"<p>Acquisition will automatically ask if you want to do/redo HPI fit every time you click START or RESTART. If you pressed Skip but want to do the HPI fit anyway, you need to restart the recording.</p> <p>Problem: If Acquisition does not ask about measuring HPI, it might be because it has not registered the HPI digitization. You can check if HPI is digitized in the Acquisition main window. It will either specify the time the HPI was digitized or say \"HPI: not digitized!\"</p> <p>Solution: load the correct preparation with the digitized HPI coil locations</p> <p>! Note that every time you click start in Acquisition, it saves a new preparation, so make sure that you note the time you saved the preparation with the digitized HPI on the digitization PC.</p>"},{"location":"natmeg/preparation/02_Digitization-hpi/#hpi-errors","title":"HPI errors","text":"<p>(HPI results window showing successful HPI fit)</p> <p>Problem: Errors with HPI. It gives an error message or Suggestion in HPI results window (screenshot) is \"redo HPI\" as opposed to \"Accept\" (like in the screenshot).</p> <p>Solution: 1. Make sure the correct preparation is loaded with the digitized HPI coil locations. 2. Check that the HPI cable is connected to the panel at the side of the scanner. 3. Click \"Try again\" to see if the fit has improved. 4. Make sure that all HPI coils on the participant's head is inside the helmet. If possible, reposition the participant, so at least three HPI coils are inside the helmet. Click \"Try again\" to see if the fit has improved. If not, proceed to step six. 5. Check for loose coils that might have come off. Do not try to re-attach them; go to step six. 6. Redo HPI/isotrak fit: Get the participant out of the scanner to the preparation area. Then reposition/re-attach HPI coils and redo the digitization.</p>"},{"location":"natmeg/presentation/01_Presentation/","title":"Presentation","text":"<p>For an extended manual on Presentation software, see Neurobs help page, this page is only for NatMEG specific configurations.</p> <p>Some tips and tricks are available in Presentation-script-tips</p> <p>Example scripts:  * Resting state paradigm * Auditory oddball paradigm</p>"},{"location":"natmeg/presentation/01_Presentation/#setting-up-the-response-buttons","title":"Setting up the response buttons","text":"<ol> <li>Add the input ports you want to your experiment.</li> </ol> <p>You need an input port which is port6 for the fORP-buttons response pads. Select the Response Device and configure the port settings.</p> <p></p> <ol> <li>Add output ports</li> </ol> <p>Here port0 is Button pad 1 and port4 is Button pad 2</p> <p></p> <ol> <li>Add response buttons</li> </ol> <p>Here you need to add active buttons to the experiment, both from  the Button pads and from the keyboard, should you need it.</p> <p></p> <p>\\blandscape \\small</p>"},{"location":"natmeg/presentation/01_Presentation/#ports-overview","title":"Ports overview","text":"DIGITAL LINES Port num Port name Port function Comment Bit line 1 Bit line 2 Bit line 3 Bit line 4 Bit line 5 Bit line 6 Bit line 7 Bit line 8 PCIe-6509 0 MEG (1-8) Transmit: Event triggers to MEG acquisition system Connects to STI101, lines 1-8 (STI001-008) STI001 STI002 STI003 STI004 STI005 STI006 STI007 STI008 PCIe-6509 1 AudioFile Transmit AudioFile sound stimulator Valve 1 ch1 Valve 1 ch2 Valve 1 ch3 Valve 1 ch4 Valve 2 ch6 Valve 2 ch7 Valve 2 ch8 Valve 2 ch9 PCIe-6509 2 CCS trig + AF trig Transmit Triggers CCS stimulators 1 and 2, Triggers AudioFile BNC 1 Aux 3 BNC 2 Aux 4 BNC 3 Line 1 PCIe-6509 3 PAM Transmit Pneumatic valves; PAM, membranes, brush robot Valve 1 ch1 Valve 1 ch2 Valve 1 ch3 Valve 1 ch4 Valve 2 ch6 Valve 2 ch7 Valve 2 ch8 Valve 2 ch9 PCIe-6509 4 MEG (9-16) Transmit: Event triggers to MEG acquisition system Connects to STI101, lines 9-16 (STI009-016) STI009 STI010 STI011 STI012 STI013 STI014 STI015 STI016 PCIe-6509 5 RealTime Receive Connected to LPT5 for MatLab &gt; Presentation interaction Bit 1 Bit 2 Bit 3 Bit 4 Bit 5 Bit 6 Bit 7 Bit 8 PCIe-6509 6 fORP Receive 2x4 response pads Connects to STI102, ch 9-16 Left Blue STI102 Ch 9 Left Yellow STI102 Ch 10 Left Green STI102 Ch 11 Left Red STI102 Ch 12 Right Blue STI102 Ch 13 Right Yellow STI102 Ch 14 Right Green STI102 Ch 15 Right Red STI102 Ch 16 PCIe-6509 7 FIB OPT Receive Ch 1-5 Fiber-optic gadgets Connects to STI102, ch 1-5 STI102 Ch 1 STI102 Ch 2 STI102 Ch 3 STI102 Ch 4 STI102 Ch 5 STI102 Ch 6 (not used) PCIe-6509 7 FIB OPT Receive Ch 7-8 CCS stimulation, Connects to STI102, ch 7-8 CCS1 STI102 Ch 7 CCS2 STI102 Ch 8 LPT 1 EyeLink Transmit: Event triggers to EyeLink acquisition system Bit 1-8 Bit 1 Bit 2 Bit 3 Bit 4 Bit 5 Bit 6 Bit 7 Bit 8 LPT 5 MatLab/ Realtime Transmit Connected to PCIe-5 for MatLab &gt; Presentation interaction Bit 1 Bit 2 Bit 3 Bit 4 Bit 5 Bit 6 Bit 7 Bit 8 AUX/ MISC 1 AUX/ MISC Transmit/ Receive BNC link btw Stimulation and MSR cabinet Aux 3 Aux 4 Aux 5 Aux 6 Aux 7 Aux 8 Aux 9 Aux 10 <p>\\elandscape</p>"},{"location":"natmeg/presentation/02_Presentation-script-tips/","title":"Presentation script tips","text":""},{"location":"natmeg/presentation/02_Presentation-script-tips/#code","title":"Code","text":"<p>The Presentation code is made up of SDL and PCL languages. Basically, in SDL you define objects that will be shown on the screen.</p> <p>In PCL you do the workflow of your experiment. You can also change parameters of objects created in the SDL part.</p>"},{"location":"natmeg/presentation/02_Presentation-script-tips/#python-in-presentation","title":"Python in Presentation","text":"<p>In PCL you can run separate Python (or other) scripts using the <code>run_process()</code> function. You can for example run a python-file for image processing. The first argument is the <code>python</code> command, second is the file to be run. The third option is whether to run the script in parallel in the background while continuing with the Presentation code, or wait for the Python code to complete before moving to the next line in Presentation. If <code>true</code>, note that if you want to use the outcome of the python-script in a subsequent part of your Presentation script you need enough time to finish it (or simply set it to <code>false</code>).</p> <p>The final argument is whether to hide or show user interface.</p> <pre><code># Process will run and Presentation will pause until finished\nrun_process('python', 'process_image.py', false, true)\n</code></pre>"},{"location":"natmeg/presentation/02_Presentation-script-tips/#presentation-in-python","title":"Presentation in Python","text":"<p>! NatMEG core team has not tried all functions so this is to be considered experimental for now.</p> <p>Presentation has a python module called PresPy. With it you can program in python and execute Presentation commands.</p> <pre><code>import sys\nif sys.version_info.major &lt; 3:\n   sys.path.append( r\"C:\\Program Files (x86)\\Neurobehavioral Systems\\Presentation\\Extensions\\Python\\2.7\" )\nelse:\n   sys.path.append( r\"C:\\Program Files (x86)\\Neurobehavioral Systems\\Presentation\\Extensions\\Python\\3.7\" )\n\n\nimport PresPy\n\npc = PresPy.Presentation_control()\n\npc.open_experiment(r\"D:\\NATMEG PROJECTS\\&lt;PATH TO YOUR EXP FILE&gt;\" )\npc.set_header_parameter( \"active_buttons\", 9 )\npc.set_header_parameter( \"button_codes\", \"1,2,3,4,5,6,7,8,9\" )\n\nscen = pc.run( 0 )\n\nbitmap = scen.bitmap() # Create a bitmap stim\ntext = scen.text() # Create a text stim\npic = scen.picture() # Create a picture stim\ntrial = scen.trial() # Create a trial\n\npic.add_part(bitmap, 0, 0)\n\ntrial.add_stimulus_event(pic)\ntrial.add_stimulus_event(text)\n\nclock = scen.get_var('clock')\n\nt = clock.time_double()\ntrial.present()\nprint(clock.time_double() - t)\n</code></pre> <p>More to come...</p> <p>For an extended manual on Presentation software, see Neurobs help page.</p>"},{"location":"natmeg/presentation/Examples/Presentation-code-example-Auditory-oddball-paradigm/","title":"Auditory oddball paradigm","text":""},{"location":"natmeg/presentation/Examples/Presentation-code-example-Auditory-oddball-paradigm/#section-0-introduction","title":"Section 0: Introduction","text":"<p>This code example is very similar to the other. However, here we introduce 3 new things:</p> <ul> <li>new stimulus: sound. Notice that this stimulus is not defined in the SCE code because we get the sound from the sound files (you can play them)</li> <li>we add a practice block, which is not interesting to record, but it is needed to be sure that the participant has understood the task</li> <li>in the previous one, there was a resting state where nothing was happening meanwhile. In this case though, in each block (there are 2) different sounds are played. Thus, we have a .txt file where each row corresponds to a trial. A trial is basically a group of sounds that will be played and have the same tone and at the end of the trial, a sound with a lower or higher tone will be played. Thus, we store the information of this .txt file in an array variable. Thus, in this case, the lenght of your experiment is determined by the lenght of the .txt file and the duration of each sound per trial. </li> </ul> <p>Notice that here we will use the same subroutines functions and the same stimulus defined in the SCE code</p>"},{"location":"natmeg/presentation/Examples/Presentation-code-example-Auditory-oddball-paradigm/#section-1-initial-parameters","title":"Section 1: initial parameters","text":"<p>You need to think about what parameters are important for the interface of your experiment. </p> <ul> <li>First of all, if you want to show any text instructions you may want to have different language options</li> <li> <p>In the case of this experiment, we also added a block for the participant to practice first. However, since the experiment was carried out 2 times per participant, in the second time it was not necessary to do the practice exercise. Think if in your experiment code there are blocks for which you need dinamic changes (go over or not go over this block for example) across different experiments with the same or different subjects. If that is the case you need parameters to control tese changes more easily. For example, in the case of this code, instead of comenting the practice exercise block every time we run it (which would be not very practical neither time-efficient), we have a parameter that we include in a conditional statement to decide if we want to execute that part of the code or not. </p> </li> <li> <p>extra needed subroutines</p> </li> <li> <p>ports</p> </li> </ul> <pre><code>##################### PARAMETER THAT CAN BE CHANGED BETWEEN EXPERIMENTS #######################\n# Add parameter that you may want to change between experiments, so that you get easy access to them\nstring language = \"eng\"; #language can be \"eng\" or \"sv\"\nstring PracticeYes = \"y\";  #ONLY set this to no when you want to skip the practice block in piloting \"y\" or \"n\"\n\n\n##################### FILES THAT NEED TO BE INLCUDED #######################\n# First of all, we need to include all the necessary files that will be used in this experiment\ninclude \"subroutines.pcl\";\"``\n\nbegin; #beggining of our experiment\n\n##################### PORTS DEFINITIONS #######################\n# in the app to manage the exeriments, there are output ports. We need to know this\n# so that the output of the code goes to the correct output port of the system\nint AFport = 2;\nint myMEGport = 1;\noutput_port audiofile = output_port_manager.get_port(AFport);\noutput_port MEGport = output_port_manager.get_port(myMEGport);\noutput_port OPMport = output_port_manager.get_port(3);\n\n\ndefault.present(); #this function presents the settings in the screen.\nparameter_window.remove_all(); #This function removes any previously configured parameters from the parameter window to start again a new experiment.\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation-code-example-Auditory-oddball-paradigm/#section-2-prepare-your-experiment","title":"Section 2: Prepare your experiment","text":"<p>In order to start running the experiments, you may need to define different variables like strings or lists or counters. </p> <p>Let's analyse this experiment: This is an auditory oddball experiment were basically some sounds are reproduced. Most of these sounds have a certain \"tone\" and some specific ones have lower or higher tones in comparison with the standard tone (the one that the majority of the sounds have). So in this experiment, each trial will be something like: beep, beep, beep, beep_higher.</p> <p>For this experiment we have a .txt file with 3 colums and 60 rows (which is the total number of trials we run per block in a single experiment). In the first colum we define the trial number (1 to 60), in the second colum the number of normal balls before the lower or higher oddball is reproduced and in the third colum a number that can be either \"low\" (lower tone) or \"high\" (higher tone). This experiment has 2 blocks, one where the go oddball is high (so the participant needs to press the button when they hear the higher tones) and the no-go oddball is low, and then in the second block is the other way around. </p> <p>An example row of this .txt file is the following: 1   3   high</p> <p>This is trial 1, with 3 normal balls and 1 high oddball. </p> <p>Since the participant may not fully understand the task from our explanation, we also have a practice block (no recording needed for that).</p> <p>Thus, in this case we need to define the following variables: </p> <ul> <li> <p>PracticeRunTrials where we will store the definition of each trial in the practice section by using our .txt file \"NatMEG_Auditory_trial_practicerun\"</p> </li> <li> <p>EXPtrials where we will store the definition of each trial in the experiment section by using our .txt file \"NatMEG_Auditory_trials\"</p> </li> <li> <p>then we define the frequency of the sounds we will reproduce based on their category (noram ball, low oddball or high oddball). We have an aodio file where each number repesents different sounds with different frquencies.</p> </li> <li> <p>Also, when you record your data and you know that something is happening, you may want to set some triggers to actually know when those things are happening. Imagine you show an image to the participant during 3 seconds at some point in your experiment. You will probably record more data at once, not just the 3 seconds. Thus, along the timeframe of your recorded data, you may want to know when the image appeared and the image was gone (to for example further analyse that particular part of the data or to know that if your data looks different in that time frame its due to the image that you were showing to the participant). For that, we use triggers. This triggers will be send to the recording MEG and OPM systems. We will further analyse this in other parts of the code.</p> </li> <li> <p>Also, if you have dynamic variables that will change during the code (they are not fixed) in if conditions, it is better to first define and initialize them, because otherwise the code will give an error.</p> </li> </ul> <pre><code>int trialBlockLen = 6;  #Total number of trials per block. This is the total number of  rows in our file (each containing 1 oddball).\ninput_file PracticeRunFile = new input_file; # prepare out input file\nPracticeRunFile.open(\"NatMEG_Auditory_trial_practicerun.txt\"); # define the input file\n\n#In this loop we are reading the file where the trials are defined and storing those values in a variable \n#since PracticeRunTrials is an array and arrays need to have the same type of data, we need to devide them into 2 different arrays\narray&lt;int&gt; PracticeRunTrials[trialBlockLen][2]; # make an array with trialblocklen rows and 2 columns (with integers)\narray&lt;string&gt; PracticeRunTrials2[trialBlockLen][1];  # make an array with trialblocklen rows and 1 column (with strings)\nloop int r=1 until r&gt; trialBlockLen begin\n    PracticeRunTrials[r][1] = PracticeRunFile.get_int(); # read all the columns from the practice file into the array\n    PracticeRunTrials[r][2] = PracticeRunFile.get_int();\n    PracticeRunTrials2[r][1] = PracticeRunFile.get_string();\n    r=r+1;\nend;\nPracticeRunFile.close(); #close file\n\n\n#Now the same but with the experiment section\n\nint blockLen = 60; #Total number of trials per block. This is the total number of  rows in our file (each containing 1 oddball).\ninput_file myfile = new input_file; \nmyfile.open(\"NatMEG_Auditory_trials.txt\");  #opening the file where the 60 trials are defined\n\narray&lt;int&gt; alltrials[blockLen][2]; #alltrials will store all the info of the 60 trials are defined\narray&lt;string&gt; alltrials2[blockLen][1];\nmyfile.open(fname); \nloop int r=1 until r&gt; blockLen begin\n    alltrials[r][1] = myfile.get_int();\n    alltrials[r][2] = myfile.get_int();\n    alltrials2[r][1] = myfile.get_string();\n    r=r+1;\nend;\nmyfile.close(); #closing the file\n\n\n#we have an aodio file where each number repesents different sounds with different frquencies\n#The soundNum variables will tell us which sound should we pick for the normal beep, low aoddball and high oddball\n\nint NormalBeepSoundNum = 37; # sound number 37=39_B4 -&gt; 987 Hz\nint LowOddballBeepSoundNum = 75; #1; sound number 75= 43_G4 -&gt; 782 Hz\nint HighOddballBeepSoundNum = 79; #2; sound number 79= 43_D5 -&gt; 1173 Hz\n\n#Also, for post-processing of data, we need to know when a normal or a high oddball or a low odball happend.\n#For this, we label the sound with a number (called trigger) to send it to the system so that afterwards we can identify when the stimulus occured\n\nint NormalBeepTrigger = 1; #trigger code for normal beep (1 for sound onset)\nint OddballBeepTrigger = 3; #2#trigger code for no-go oddball (2 for no-go, 1 for sound onset)\nint GoOddballBeepTrigger = 5; # trigger code for go oddball (4 for go, 1 for sound onset)\nint beepTrigger = 0;\n\nint ITI = 1250; #1250 ITI ms ????????????\n\n#some variables that need to have a defoult inicialization before using them\nint WhichOddball = 1; # Initial identification of the go or not go oddballs 1: NoGoOddball or 2: GoOddball\nint beepTrigger = 0;\nint soundNum = 0;\n\n\narray&lt;string&gt; GoOddBallFrequency[2][2];  # create empty array with two rows and two col\nGoOddBallFrequency[1][1] = \"high\";  #assing values to the empty array\nGoOddBallFrequency[2][1] = \"low\";\n\nif language == \"sv\" then\n    GoOddBallFrequency[1][2] = \"h\u00f6gre\";  #assing values to the empty array in swedish in column 2\n    GoOddBallFrequency[2][2] = \"l\u00e4gre\";\nelse\n    GoOddBallFrequency[1][2] = \"higher\";  #assing values to the empty array in swedish in column 2\n    GoOddBallFrequency[2][2] = \"lower\";\nend;\n\n# This line is used to suffle the orther when deciding the GO ODBALL of the block. So there si a 50% change that the first GO oddball is high \n# and 50% of being low\nGoOddBallFrequency.shuffle();  # randomize order of High and Low as the go-oddball\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation-code-example-Auditory-oddball-paradigm/#section-3-practice-section-for-the-participant","title":"Section 3: Practice section for the participant","text":"<p>In every practice exercise or experiment we have 2 loops:</p> <ul> <li> <p>The first loop will correspond to the block number. Every exercise or experiment will have 2 blocks: 1 block for the high odball and 1 block or the low oddball (the order of these 2 blocks is random)</p> </li> <li> <p>The second loop will correspond to the trial number inside 1 block. As explained before, the practice exercise has 6 trials per block and the experiment has 60 trials per block</p> </li> </ul> <p>Since this is just a practice exercise, we are not using the triggers for the MEGport or OPMport. The general structure of this practice exercise is the following:</p> <ul> <li> <p>loop through each block</p> <ul> <li>Show instructions to the participant explaining which is the go oddball in this block (high or low) in the correspoding language</li> <li>Shuffle the order of the tirals</li> <li> <p>Present the fixation cross</p> </li> <li> <p>loop through each trial</p> <ul> <li>Take the oddball type for that corresponding trial (this is the WhichOddball variable): low or high</li> <li>Take the total number of normal beeps that the participant will hear before the oddball in that corresponding trial</li> <li>(Meanwhile show the progress in our screen)</li> <li> <p>compare the current oddball type with the go odball in the block to know if the current oddball is a \"go\" or \"no-go\" </p> </li> <li> <p>loop to play the normal beeps (determined by the previous step)</p> <ul> <li>Get the sound from the audio port (with the corresponding frequency)</li> <li>wait interval</li> <li>Play/trigger the sound</li> <li>Wait interval</li> </ul> </li> <li> <p>After the normal beeps, play the corresponding oddball sound</p> </li> </ul> </li> </ul> </li> </ul> <pre><code>if PracticeYes == \"y\" then\n    loop int PracticeBlockCounts = 1; until PracticeBlockCounts &gt; 2 begin; #loop per each block\n\n            # INSTRUCTIONS: define and show which is the GO odball of this block\n            if language == \"sv\" then\n                ShowInstructions(\"En kort \u00f6vning innan experimentet startar \\n\\n\",\n                \"Tryck p\u00e5 knappen n\u00e4r du h\u00f6r den \\n\\n \", \n                GoOddBallFrequency[PracticeBlockCounts][2] + \" tonen\");\n            else\n                ShowInstructions(\"A short exercise before the experiment starts \\n\\n\", \n                \"Press the button when you hear the \\n\\n \", \n                GoOddBallFrequency[PracticeBlockCounts][2] + \" tone\");\n            end;\n\n            PracticeRunTrials.shuffle(); #shuffle the order \n\n            p_Fixation.present();# present fixation cross\n\n            loop int MyCount=1; until MyCount &gt; trialBlockLen begin #loop per each trial, 60 in total. \n            #MyCount controls the trial number\n\n                WhichOddball = PracticeRunTrials2[MyCount][1]; # take the current oddball\n                int TotalNormalBeeps = PracticeRunTrials[MyCount][2]; #total number of normal beeps before oddball\n\n            #if the oddball of this trial is of the same type as the go oddball of the block, \n            #then it is a go oddball\n                if WhichOddball == GoOddBallFrequency[PracticeBlockCounts][1] then\n                    ball_textTRIAL = \"go\";\n                else # else, it is a no-go odball\n                    ball_textTRIAL = \"no-go\";\n                end;\n\n                # show progress in our screen\n                ShowAltText(\"Block: \" + string(PracticeBlockCounts) + \"  the go-oddball is \" + \n                GoOddBallFrequency[PracticeBlockCounts][1], \"n of OddBalls: \" + string(MyCount) + \"/\" + \n                string(trialBlockLen), \"Odball type: \" + ball_textTRIAL);\n\n                # Present normal beeps\n                loop int normalBeeps=1; until normalBeeps &gt; (TotalNormalBeeps) begin\n\n                    audiofile.send_code(NormalBeepSoundNum,80); #load sound\n                    wait_interval(50); #wait interval\n                    audiofile.send_code(128,10); #trigger sound\n                    wait_interval(ITI); # wait interval\n                    normalBeeps = normalBeeps + 1\n                end;\n\n                #sound that needs to be played for the odball knowing if it is a low or high oddball\n                if WhichOddball == \"low\" then  #low oddball\n                    soundNum = LowOddballBeepSoundNum;\n                else    #high odball\n                    soundNum = HighOddballBeepSoundNum;\n                end;\n\n                audiofile.send_code(soundNum,80); #load sound\n                wait_interval(50); # wait interval      \n                audiofile.send_code(128,10); #trigger sound\n                wait_interval(ITI); # wait interval\n\n                #Update count\n                MyCount = MyCount+1;\n\n            end; #trial loop\n            PracticeBlockCounts = PracticeBlockCounts + 1;  \n        end;    \nend;\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation-code-example-Auditory-oddball-paradigm/#section-4-experiments","title":"Section 4: experiments","text":"<p>This code follows exactly the same structure, however this time there are more trials per block and each time we play a sound, we also send a trigger to the MEGport and the OPMport so that when we analyse our data we can know when each sound was played. </p> <pre><code>########################## START EXPERIMENT ######################################################################################\n\nloop int block=1; until block &gt; 2 begin;\n\n    # suffle all the data\n    alltrials.shuffle();\n    alltrials2.shuffle();\n\n    # ask sleppiness to the participant\n    kss_routine(language);\n\n    #show instructions and define the GO ODDBALL (high or low). Remember that GoOddBallFrequency was also shuffled before\n    if language == \"sv\" then\n        ShowInstructions(\"Tryck p\u00e5 knappen n\u00e4r du h\u00f6r den \\n\\n \", GoOddBallFrequency[block][2] + \" tonen\", \" \");\n    else\n        ShowInstructions(\"Press the button when you hear the\\n\\n \", GoOddBallFrequency[block][2] + \" tone\", \" \");\n    end;\n\n    p_Fixation.present(); #present fixation cross\n\n    # loop per each trial\n    loop int MyCount=1; until MyCount &gt; blockLen begin\n\n        WhichOddball = alltrials2[MyCount][1]; #take the oddball of this trial\n        int TotalNormalBeeps = alltrials[MyCount][2]; # take the total number of normal beeps before the odball in this trial\n\n        if WhichOddball == GoOddBallFrequency[block][1] then #check if it is a go or no-go odball\n            ball_text = \"go\";\n            beepTrigger = GoOddballBeepTrigger;\n        else\n            ball_text = \"no-go\";\n            beepTrigger = OddballBeepTrigger;\n        end;\n\n        #show progress in our screen\n        ShowAltText(\"Block: \" + string(block) + \"  the go-oddball is \" + GoOddBallFrequency[block][1], \"n of OddBalls: \" + string(MyCount) +\n\"/\" + string(blockLen), \"Odball type: \" + ball_text);\n\n        # Present normal beepp\n        loop int normalBeeps=1; until normalBeeps &gt; (TotalNormalBeeps) begin\n\n            audiofile.send_code(NormalBeepSoundNum,80); #load sound with normal beep sound num\n            wait_interval(50);\n            MEGport.send_code(NormalBeepTrigger,40); # send predefined trigger\n            OPMport.send_code(NormalBeepTrigger,40); # send predefined trigger\n            audiofile.send_code(128,10); #trigger sound (play it)\n\n            wait_interval(ITI); #Not random ITI\n            normalBeeps = normalBeeps + 1\n        end;\n\n        # Present oddball\n        if WhichOddball == \"low\" then  #low oddball\n            soundNum = LowOddballBeepSoundNum;\n            beepTrigger = beepTrigger + 0;\n        else    #high odball\n            beepTrigger = beepTrigger + OddBallFrequencyHighTrigger; # if the go-oddball is low, then the no-go oddball is high\n            soundNum = HighOddballBeepSoundNum;\n        end;\n\n        audiofile.send_code(soundNum,80); #load sound\n        wait_interval(50);\n        MEGport.send_code(beepTrigger,40);   # send our updated beepTrigger\n        OPMport.send_code(beepTrigger,40); # send predefined trigger\n        audiofile.send_code(128,10); #trigger sound\n        wait_interval(ITI); #Not random ITI\n\n        #Update count\n        MyCount = MyCount+1;\n\n    end; #trial loop\n    block = block + 1;\n\nend; # block loop\nkss_routine(language);  \n\nendOfTest.present();\nend;\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/","title":"Resting state paradigm example","text":""},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-0-introduction","title":"Section 0: Introduction","text":"<p>This provided code example is for a resting state recording. The code presents a fixation cross while the MEG system records the brain activity. This experiment has 2 blocks: one where the participant needs to be with the eyes open and another where the participant needs to be with the eyes closed.</p>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-1-pcl-subrutines","title":"Section 1: PCL subrutines","text":"<p>Presentation, as most of any other programming languages, has predetermined functions that we can use. For example: ----.present(); will display something, perhaps an image, on the screen.</p> <p>However, in this code you will notice that other functions used are not from presentation per se. There is a .pcl file in the same folder that has different functions defined that will be used in this main code. These codes provide what are called \"subroutines\", which are a certain sequence of operations which form a whole function and those are put together forming a little module. In this case the functions are: - kss: In our code we use kss() to show the sleepiness questions to the participant. - ShowInstructions: We use ShowInstructions() to show text instructions to the participant on the screen. - ShowAltText: We use ShowAltText() to show a certain text on our screen (not the participant's screen). - WaitForAllButtonpress: Used to wait for any button press in the keyboard to continue with the code.</p> <p>It is always good practice to organize the code in this way: main code + functions/subroutines. In this way, if there is a set of instructions that you know you will use repeatedly in your code/codes, you can create a function to perform that task and then call the function. For example: during my experiment I will need to evaluate the sleepiness of my participant at least 3 times, then it is a good opportunity to write a function that you can re-use. </p>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-2-exp-files","title":"Section 2: .exp files","text":"<p>The experiment file is a file that helps to establish connections between the software and the hardwere. The main examples are:</p> <p>(1) Define available buttons from keyborad: The number of buttons and type of button that the user can click in the keyboard for the code to identify them will be defined in this file. Each button that the user can press in the keyboard will be mapped to a number. For example, if the user clicks the button for number one 1 in the keyboard (that is the button name), that will be button 1 in the code. If the user clicks enter in the keyboard, that will be button 12 in the code.</p> <p>(2) Defining the input and output ports. The number and type of input and output ports needs to be defined. Each port will be again associated with a number. In our case we have a card with ports which is called PCle-6509 and we know that port0 slot in that card corresponds to the MEG port. Thus, we will assign this port with a number, in this case number 1, and in the code we will use that number to refer to the MEG_port. </p>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-2-sce-code","title":"Section 2: SCE code","text":"<p>You will notice that in the resting state folder there are .pcl codes and .sce codes. PCL codes are used to control your experiment in general; Blocks that it will have, when I am going to present something, send triggers to the recording systems, among others. SCE files are used to define certain stimules that afterwards will be used and displayed in the PCL code. For example: image, video, sound, and others.</p> <p>In this resting state example, there 2 main stimulus that we defined in the SCE code and that now will be explained.</p> <p>First is also important that you notice some variables used for default settings of text and backgorund and also including files that are needed for the code:</p> <pre><code>scenario = \"CAPSI_resting_state\"; \npcl_file = \"CAPSI_resting_state.pcl\";\n\ndefault_background_color = 0,0,0; # black\ndefault_text_color = 255,255,255; # white\n\ndefault_font = \"arial\";\ndefault_font_size = 48;\ndefault_text_align = align_left;\n\nThis are the buttons from the keybord to be inluded\nactive_buttons = 15;\nbutton_codes = 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15;\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#fixation-cross-image","title":"Fixation cross image","text":"<p>If you go to the SCE file you will see the following:</p> <pre><code>#fixation cross (in color black)\npicture {\n    box { height = 5; width = 60; color = 255,255,255; } b_Horiz; x = 0; y = 0;\n    box { height = 60; width = 5; color = 255,255,255; } b_Verti; x = 0; y = 0;\n} p_Fixation;\n</code></pre> <p>In this case we define a stimules which is a picture. The default background colour is black and on top of our background we will show this picture. This picture consits on 2 rectangles, which are the same but oriented differently (notice that height and width are swithced), thus forming a cross. Both of the rectangles are white so the cross is white.</p> <p>It is important to notice that \"picture {\" in the beggining defines the type of stimulus that your will define and \"} p_Fixation;\" in the end is the name that you give to this stimulus so that afterwards you can call it in the main PCL code. See the presentation manual for further explanation on how to define different stimulus.</p>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#text-in-the-screen","title":"Text in the screen","text":"<p>To show text in the screen, we have a function in the subroutines PCL file and also a stimulus defined in this SCE code. These two functions are connected to show text in the screen in the main PCL file. Now we will explain the subroutines function. </p> <pre><code>ShowAltText is a function defined in the subrountines file:\n\nsub ShowAltText( string messageA, string messageB, string messageC )\nbegin\n  t_Alt3La.set_caption( messageA );\n  t_Alt3La.redraw();\n  t_Alt3Lb.set_caption( messageB );\n  t_Alt3Lb.redraw();\n  t_Alt3Lc.set_caption( messageC );\n  t_Alt3Lc.redraw();\n  p_Alt3L.present();\nend;\n</code></pre> <p>This funtion always needs 3 inputs (3 strings to be more specific). Thus, you always need to call the function as:  <pre><code>ShowAltText(\"something\", \"something\", \"something\");\n</code></pre></p> <p>Then, notice that in the SCE file there is this code:</p> <pre><code>#alt screen (3 lines)\npicture {\n    display_index = 2; \n    text { display_index = 2; caption = \" \"; } t_Alt3La; x = 0; y = 80;\n    text { display_index = 2; caption = \"Press [ENTER] to confirm or [Esc] to abort. \"; } t_Alt3Lb; x = 0; y = 0;\n    text { display_index = 2; caption = \" \"; } t_Alt3Lc; x = 0; y = -80;\n} p_Alt3L;\n</code></pre> <p>This stimulus is a picture, however, instead of having 2 rectangles, it has text. The text that will be shown is: t_Alt3La, t_Alt3Lb and t_Alt3Lc. These three text variables are the same that were defined in the ShowAltText() function (they have the same names). Thus, the ShowAltText() funtion receives as inputs 3 strings that will be used to define the text string that our stimulus will use to present. </p> <p>To show instructions to the participant, we also use a very similar method but with a different function and different stimulus. This is the function in the subroutines folder:</p> <pre><code>sub ShowInstructions( string messageA, string messageB, string messageC )\nbegin\n  t_Info3La.set_caption( messageA );\n  t_Info3La.redraw();\n  t_Info3Lb.set_caption( messageB );\n  t_Info3Lb.redraw();\n  t_Info3Lc.set_caption( messageC );\n  t_Info3Lc.redraw();\n  p_Info3L.present();\n  WaitForAllButtonpress();\nend;\n</code></pre> <p>And this is the stimulus:</p> <pre><code>#info screen (3 lines)\npicture {\n    text { caption = \" \"; } t_Info3La; x = 0; y = 80;\n    text { caption = \"Press [ENTER] to confirm or [Esc] to abort. \"; } t_Info3Lb; x = 0; y = 0;\n    text { caption = \" \"; } t_Info3Lc; x = 0; y = -80;\n} p_Info3L;\n</code></pre> <p>The main difference is that by default, this text will be shown in the participant's screen. When we need text in our screen we need to specify display_index = 2; as we did in the stimulus for showing text in our screen. </p>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-3-initial-parameters-main-code","title":"Section 3: Initial parameters (MAIN CODE)","text":"<p>You need to think about what parameters are important for the interface of your experiment. </p> <ul> <li> <p>First of all, if you want to show any text instructions you may want to have different language options.</p> </li> <li> <p>Then, in order to run your experiments, you probably will need extra files. It is good practice that you split your code in different parts and write a \"main\" file that helps you to control all the experiment. </p> </li> <li> <p>Ports: Presentation can handle different external devices. Ports can be used to send data markers / triggers so that stimuli events can be synchronized from your Presentation script with other devices, for example a MEG recording. When a marker is sent from Presentation, recorded time is synchronized with the external device. A data marker is defined by a number ranging between 1 and 255. This marker is sent to your data recording system, and represents a single data point within your recorded data. Every event that you are interested in synchronizing within your Presentation script can be associated with an individual marker, such that you can easily process your data after recording. In this case the output ports where we want to send triggers for data synchronization are: the MEG port, the OPM port (so both MEG systems we are currently studying).</p> </li> </ul> <pre><code>##################### PARAMETER THAT CAN BE CHANGED BETWEEN EXPERIMENTS #######################\n# Add parameter that you may want to change between experiments, so that you get easy access to them\nstring language = \"eng\"; #language can be \"eng\" or \"sv\"\n\n\n##################### FILES THAT NEED TO BE INLCUDED #######################\n# First of all, we need to include all the necessary files that will be used in this experiment\ninclude \"subroutines.pcl\";\n\nbegin; #beggining of our experiment\n\n##################### PORTS DEFINITIONS #######################\n# in the app to manage the exeriments, there are output ports. We need to know this\n# so that the output of the code goes to the correct output port of the system\nint myMEGport = 1;\noutput_port MEGport = output_port_manager.get_port(myMEGport);\noutput_port OPMport = output_port_manager.get_port(3);\n\n\ndefault.present(); #this function presents the settings in the screen.\nparameter_window.remove_all(); #This function removes any previously configured parameters from the parameter window to start again a new experiment\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-4-prepare-your-experiment","title":"Section 4: Prepare your experiment","text":"<p>In order to start running the experiments, you may need to define different variables like strings or lists or counters. </p> <p>In this experiment, we mainly want to show a fixation cross while recording the user in this resting state. We need to repeat this for 2 blocks: one where the participant needs to be with the eyes open, and another one with the eyes closed.  The main variable we need to define in this case are:</p> <ul> <li>exp_duration: control the total lenght of each block</li> <li>RestingStates: It is a string type of array where the first column is used so that WE know which block are we recording and the second column is for the participant to follow the corresponding instructions depending on the block type. </li> </ul> <p>Normally, it is useful to have an array type of variable so that you control which block are you recoding and thus, also show the correspoding instructions to the participant. If you do not run different blocks (for example, you just do eyes closed), then, this is not necessary.</p> <pre><code>##################### PREPARING YOUR EXPERIMENT #######################\nint exp_duration = 30; # Total time: 2 blocks, 5 minutes per block (300 seconds each)\n\n\n## Add definitions of resting state eyes open and eyes closed\n\narray&lt;string&gt; RestingStates[2][2];  # create empty array with two rows and 2 col\n\n# THIS IS FOR US TO KNOW WHICH BLOCK ARE WE IN [X][1] (first column)\nRestingStates[1][1] = \"Eyes open\";  # THIS IS FOR US TO KNOW WHICH BLOCK ARE WE IN\nRestingStates[2][1] = \"Eyes closed\";\n\n# THIS IS FOR THE PARTICIPANT TO SHOW THE CORRESPONDING INSTRUCTIONS [X][2] (second column)\nif language == \"sv\" then\n    RestingStates[1][2] = \"\u00f6gonen \u00f6ppna under tiden, titta p\u00e5 korset,\\n och sitt stilla och avslappnat\"; \n    RestingStates[2][2] = \"\u00f6gonen st\u00e4ngda och sitt stilla och avslappnat\";\nelse\n    RestingStates[1][2] = \"eyes open meanwhile, look at the cross,\\n and sit still and relaxed\";\n    RestingStates[2][2] = \"eyes closed and sit still and relaxed\";\nend;\n</code></pre>"},{"location":"natmeg/presentation/Examples/Presentation%E2%80%90code%E2%80%90example%E2%80%90Resting%E2%80%90state%E2%80%90paradigm/#section-5-loop-for-the-resting-states-recording","title":"Section 5: Loop for the resting states recording","text":"<p>so the main structure of the code for the 2 blocks of this experiment is the following:</p> <ul> <li>loop through the 2 blocks (eyes open and eyes closed)<ul> <li>show the corresponding instructions in the corresponding language</li> <li>present the fixation cross</li> <li>wait 5 seconds before the trigger is send to the MEG and OPM ports</li> <li>show text in our screen for the progress</li> <li>send the triggers to the ports<ul> <li>loop until exp_duration per each block and show the progress and send sync triggers</li> </ul> </li> <li>send the \"off\" trigger to the ports</li> <li>show the kss questionaire at the end of the block</li> </ul> </li> </ul> <pre><code>###&gt;&gt; Resting state block ******************************************************************************************************\nloop int block=1; until block &gt; 2 begin;\n\n    if language == \"sv\" then\n        ShowInstructions(\" Under kommande sex minuter kommer vi att m\u00e4ta aktiviteten \\n\\n\", \n                \"medans du vilar och g\u00f6r ingenting s\u00e4rskilt \\n\\n\", \"H\u00e5ll \" + RestingStates[block][2]);\n    else\n        ShowInstructions(\" For the next six minutes we will measure the activity \\n\\n\",\n                \"while resting and doing nothing in particular\\n\\n\", \"Hold on\" + RestingStates[block][2]);  \n    end\n\n    p_Fixation.present(); #present fixation cross and wait for a button press, then add 5s before sending trigger\n    wait_interval(5000); \n\n    ShowAltText(\"Recording:\", \"resting state\", RestingStates[block][1]); # show progress for us\n    MEGport.send_code(16,40); # send \"on\" trigger\n    OPMport.send_code(16,40); # send \"on\" trigger\n    loop int dsec=1; until dsec &gt; exp_duration begin; # 30*10 sec = 5 min\n        wait_interval(10000);   #10 sec\n        MEGport.send_code(64,40); # send sync trigger\n        OPMport.send_code(64,40); # send \"sync trigger\n        ShowAltText(\" Block \" + string(block), RestingStates[block][1], \" status: \" + string(dsec) \n                + \" / \" + string(exp_duration)); # show progress for us\n\n        dsec=dsec+1;\n    end;\n    MEGport.send_code(32,40); # send \"off\" trigger\n    OPMport.send_code(32,40); # send \"off\" trigger\n    ShowAltText(\"Finished:\", \"resting state\", RestingStates[block][1]);\n\n    wait_interval(5000);\n\n    # set drivers screen to blank again\n    ShowAltText(\"\", \"\", \"\");\n\n    kss_routine(\"eng\");\n    block = block + 1;\nend; # block loop\n#kss_routine(\"eng\");    \n\nendOfTest.present();\nend;\n</code></pre> <p>So in general, when you need to do a presentation code for an experiment you need to worry about:</p> <ul> <li> <p>Variables that you may want to change in different experiments with different times/participants: example: language of instructions, a variable that controls if you want to ask or not some questions depending on the participant...</p> </li> <li> <p>ports and triggers to the MEG and OPM and maybe others.</p> </li> <li> <p>Instructions: This is a very important part since you need to make sure the participant understands the task. Think about the language, variables to store your instructions and use some functions (you can use the ones we are already using in this code example) to show the instructions in the screen.</p> </li> <li> <p>Stimuli: In this case, our only stimuli was a fixation cross on the screen. Thus, this image was defined in the .sce code. However, in other experiments you may want to set others like: sound, video, tactile sensation...</p> </li> <li> <p>Experiment structure: Think about how do you want to structure your experiments for your recordings. For this, we normally use loop structures to repeat different blocks (maybe each block has different conditions). Also, think about variables like counters or strings to store data and to control your experiment. </p> </li> </ul> <p>Normally the experiments also follow a similar pattern:</p> <ul> <li>(1) Define all your ports, include subroutines, initialize (and if needed define) the variables that you will need in your code</li> <li>(2) Prepare the first loop over the blocks of your entire experiment session</li> <li>(3) show instructions to the participant</li> <li>(4) send on triggers</li> <li>(5) loop until the end of your experiment lenght while you send sync triggers and show the progress in your screen</li> <li>(6) triggers off and finish</li> <li>(7) kss</li> </ul>"},{"location":"natmeg/response-equipment/Eye-tracker/","title":"Eye-tracker","text":"<p>For details on how to set-up Eye-tracker consult the User manual which is also available in the locker behind the MSR-cabinet.</p>"},{"location":"natmeg/response-equipment/Eye-tracker/#equipment-required","title":"Equipment required","text":"<ul> <li>The screen</li> <li>Eye-tracker</li> </ul> <p>You will also need some lines of code in your Presentation scripts to initiate the eye-tracker, start calibration procedure and start recording.</p>"},{"location":"natmeg/response-equipment/Eye-tracker/#before-measurement","title":"Before measurement","text":"<ol> <li>Put screen in correct position (see Screen and projector)</li> <li>Start eye-tracker software by pressing the letter  t  on the keyboard, then press enter</li> </ol> <p>! If you do not see a terminal on the eye-tracking PC when you turn on the screen - Check that the screen setting of the Stimulation PC is not set in dual screen mode and that the correct input channel is selected - Check that the eye-tracking computer is turned on. The computer is found at the bottom of the stimulation cabinet</p> <p>! To start eye-tracker in simulation mode type t -x and press enter</p> <ol> <li>Make sure you have the right configuration for your needs and that the corneal reflection (CR) and pupil thresholds are set.</li> <li>Adjust eye-tracker by moving the arm or by tilting the screen a bit.</li> <li>Make sure MISC-channels 1-6 are activated in Acquisition settings. This is where ET-data is stored.</li> </ol>"},{"location":"natmeg/response-equipment/Eye-tracker/#calibration-and-validation","title":"Calibration and validation","text":"<p>When participant is seated in the MSR, calibrate the system then validate your calibration.</p>"},{"location":"natmeg/response-equipment/Eye-tracker/#during-measurement","title":"During measurement","text":"<ol> <li>Monitor eye-positions</li> </ol> <p>! As participants tend to sink down a bit during recordings eye-tracking may be lost if not chair is raised properly ! If using the table, make sure pads or participant's hands are not blocking the eye-tracker</p> <ol> <li>Check calibration</li> </ol>"},{"location":"natmeg/response-equipment/Eye-tracker/#after-measurement","title":"After measurement","text":"<ol> <li>Turn off eye-tracking software</li> <li>Remove eye-tracker and put back in box</li> </ol> <p>Eye-tracking data is saved accordingly: * MISC001 = X Left * MISC002 = Y Left * MISC003 = Pupil Left * MISC004 = X Right * MISC005 = Y Right * MISC006 = Pupil Right</p>"},{"location":"natmeg/response-equipment/Eye-tracker/#specifications","title":"Specifications","text":"<p>For  eye tracking we use  an EyeLink 1000 binocular tracker from SR Research.  The current installation runs at a maximum of 1000 Hz, tracking both eyes binocular.</p> <p>The eye tracker is typically controlled via Neurobs Presentation,  Via Presentation you can access the eye tracker output in realtime.</p> <p>The X, Y  and pupil diameter of each eye is also output into the MEG system analog channels and sampled side by side with the MEG data. The calibration space of the Eye-tracker is 1920x1080, with 0,0 in the top left corner.</p> <p>The Eye tracker settings are aligned to the projector, so that the projector ini-file uses the pixel space (1920x1080), standard visual size (72x44 cm) and distance (100 cm) of the projector.</p> <ul> <li>Further technical specifications for the equipment is found in the manual here.</li> </ul>"},{"location":"natmeg/response-equipment/Eye-tracker/#issues","title":"Issues","text":""},{"location":"natmeg/response-equipment/Eye-tracker/#the-eye-tracking-program-is-beeping-and-blinking","title":"The eye-tracking program is beeping and blinking","text":"<p>The eye-tracker has been disconnected from the cables in the MSR. Reconnect the eye-tracker or close the eye-tracking program.</p>"},{"location":"natmeg/response-equipment/Eye-tracker/#eyes-look-blurry-and-the-pupils-are-not-detected","title":"Eyes look blurry and the pupils are not detected.","text":"<p>Check that the participant's hands or paddings are not blocking the camera.</p>"},{"location":"natmeg/response-equipment/Psychophysiology-specs/","title":"Psychophysiology","text":"","tags":["BioPac","psychophysiology","ECG","GSR","respiration","blood oxygenation"]},{"location":"natmeg/response-equipment/Psychophysiology-specs/#psychophysiology","title":"Psychophysiology","text":"<p>For psychophysiology measurements beyond ECG and EOG we use  an MP150 system from BioPac. In the current setup, we have modules for measurements of ECG, GSR, respiration and blood oxygenation.</p> <p>BioPac output is typically recorded via BioPac's AcqKnowledge software, but is also output into the MEG system analog channels and sampled side by side with the MEG data.</p> <ul> <li>Further information  is found here.</li> </ul>","tags":["BioPac","psychophysiology","ECG","GSR","respiration","blood oxygenation"]},{"location":"natmeg/server/install-software/Guide-for-setting-up-Freesurfer-on-Compute/","title":"Freesurfer","text":"<p>Freesurfer for doing surface-based segmentation and parcellation (e.g. used to create cortical sheets for MNE) is already installed in NatMEG's Compute server. To use Freesurfer, you need to configure your user account and set up the correct paths for your project.</p>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-Freesurfer-on-Compute/#setup-freesurfer","title":"Setup Freesurfer","text":"<p>Freesurfer is found in <code>/opt/freesurfer/7.2.0-1</code>. To configure Freesufer run the following code (or add it to your <code>.bashrc</code> configuration to run every time you start a terminal):</p> <p><pre><code>export FREESURFER_HOME=/opt/freesurfer/7.2.0-1\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n</code></pre> Now you are ready to use Freesurfer.</p>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-Freesurfer-on-Compute/#setup-project-folder-subjects_dir","title":"Setup project folder (<code>$SUBJECTS_DIR</code>)","text":"<p>Freesufer will as a default try to put output files in the folder \"/home/your_username/freesurfer\". Before you use Freesurfer, you should configure the SUBJECTS_DIR where the output files are put by Freesurfer. It is recommended to create a folder in your home directory for your project, e.g. \"home/your_username/my_project\" (but with an actual informative project name and your actual username). In that folder, you then create a folder called \"fs_subjects_dir\" for the Freesurfer data.</p> <p>You then configure the SUBJECTS_DIR by running the following code:</p> <p><pre><code>export SUBJECTS_DIR=/home/your_username/my_project/fs_subjects_dir\necho \"SUBJECTS_DIR = $SUBJECTS_DIR\"\n</code></pre> It can be an advantage to add the code snippet above to a setup script that your source every time you will work on \"my_project\".</p> <p>You are now ready to call <code>recon-all</code>.</p>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-Freesurfer-on-Compute/#further-documentation","title":"Further documentation","text":"<p>For more information on how to use Freesurfer, yous should read the Freesurfer documentation: http://freesurfer.net/fswiki/Tutorials.</p>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-R-and-RStudio-on-Compute/","title":"R and RStudio","text":"<p><code>R</code> is a statistical programming language and <code>Rstudio</code> is a development environment for R. R/Rstudio is good software for running advanced statistical analysis, e.g. statistical modelling with the <code>lme4</code> or <code>brms</code> packages and nice plotting features with <code>ggplot2</code>.</p> <p>Neither R nor RStudio is pre-installed on Compute. You have to install it yourself. At the moment it is not possible to install directly from the source. However, with minimal hacks, it can be installed through Anaconda. </p> <p>However, it appears that R/Rstudio only has little support from the developers, so this does not appear to give the latest version of R.</p> <p>For an up to date list of known issues with R/Rstudio on Compute see here.</p>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-R-and-RStudio-on-Compute/#install-rstudio-through-anaconda","title":"Install RStudio through Anaconda","text":"<p>The first step is to make sure you have Anaconda installed: https://docs.anaconda.com/anaconda/install/linux/#installation</p> <p>Make a new environment for RStudio. This will create an environment called <code>r_env</code>:</p> <p><pre><code>mamba create --name r_env\n</code></pre> Activate the environment: </p> <pre><code>source activate r_env \n</code></pre> <p>Install RStudio from Conda Forge (https://anaconda.org/conda-forge/rstudio-desktop). This will install Rstudio and all dependencies (like R itself):</p> <p><pre><code>mamba install -c conda-forge rstudio-desktop \n</code></pre> When it is done with the installation you simple write <code>rstudio</code> in the terminal and you are ready to do some fun statistics :)</p> <p>I get several error messages in the terminal when I start Rstudio. Those I ignore. Rstudio starts and works just fine.</p>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-R-and-RStudio-on-Compute/#update-r","title":"Update R","text":"<p>When starting RStudio it informs you that it is an outdated version and you should get the latest version from the website. Just click ignore. To update R your need to do it through Anaconda:</p> <pre><code>mamba update r-base\n</code></pre>"},{"location":"natmeg/server/install-software/Guide-for-setting-up-R-and-RStudio-on-Compute/#install-packages","title":"Install packages","text":"<p>Most packages can be installed as usual though R with <code>install.packages</code>. Some packages (like lme4) cannot be installed through RStudio as a default. To fix this, I needed to first install CMake. Cmake can be installed through Anaconda, like this:</p> <pre><code>mamba install -c anaconda cmake \n</code></pre> <p>After this, it was possible to install lme4 through R as usual:</p> <pre><code>install.packages(\"lme4\")\n</code></pre>"},{"location":"natmeg/server/install-software/Set-up-Anaconda-and-mamba/","title":"Set up Anaconda and mamba","text":"<p>Anaconda is an open-source and free package and environment management system which allows you to install eg. python or R packages into separate virtual environments. Miniconda is a lightweight version.</p> <p>Read here how to download and install Anaconda or Miniconda https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html</p> <p>Once you have installed and set-up Anaconda it is recommended that you use it to install mamba, basically speeding up all future installations.</p> <pre><code>conda install --channel=conda-forge --name=base mamba\n</code></pre> <p>The conda-forge channel used above is commonly used so you can save some effort by adding it to the defailt channel list.</p> <pre><code>conda config --add channels conda-forge\n</code></pre> <p>Also activate a strict priority</p> <pre><code>conda config --set channel_priority strict\n</code></pre>","tags":["Anaconda","mamba","conda","installation"]},{"location":"natmeg/server/install-software/Set-up-FieldTrip-on-Compute/","title":"FieldTrip","text":"<p>To use FieldTrip on Compute you need to download it to your <code>home</code> directory yourself. You can do this by direct download from the FieldTrup website (https://www.fieldtriptoolbox.org/download/), but the best way is to clone it directly from their GitHub page (https://github.com/fieldtrip/fieldtrip)</p> <p>Open a terminal. In your <code>home</code>directory, make a folder where you want to download FieldTrip and go to that folder, e.g.:</p> <p><pre><code>mkdir fieldtrip\ncd fieldtrip\n</code></pre> Find the link for the remote repository (on the GitHub page) and clone it. This will pull the entire repository into yout new folder:</p> <pre><code>git clone git@github.com:fieldtrip/fieldtrip.git\n</code></pre> <p>You should now have FieldTrip in your folder. To use FieldTrip, start MATLAB, go to the directory where you just cloned FieldTrip, and run <code>ft_defaults</code> in the command window.</p>"},{"location":"natmeg/server/install-software/Set-up-MNE-Python-on-Compute/","title":"MNE-Python","text":"<p>Please expand this page</p> <p>To use MNE-Python on Compute you need to install it yourself. The best way to do this is to use Anaconda.</p>"},{"location":"natmeg/server/install-software/Set-up-MNE-Python-on-Compute/#get-anaconda","title":"Get Anaconda","text":"<p>Follow the guide to install Anaconda in your Home directory: https://docs.anaconda.com/anaconda/install/linux/#installation. Skip the first part on \"Prerequisites\". </p>"},{"location":"natmeg/server/install-software/Set-up-MNE-Python-on-Compute/#get-the-specific-environment-settings-from-mne","title":"Get the specific environment settings from MNE.","text":"<p>See the installation info from MNE: https://mne.tools/stable/install/index.html</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/","title":"Tips for writing useful analysis scripts","text":"<p>Writing data analysis scripts can quickly become a mess! There are many steps to go from raw MEG/EEG data to the final results. It is essential to keep track of what processing step that goes before and after another. Know what data that should be read in one step, saved to memory, and then read in the next step. If we mess this up, we might end up with invalid results. And it is easy to  make errors: read the wrong data files, using different versions of toolboxes, working in the wrong directory, etc., especially in MEG/EEG data processing where there are several manual steps and we often have to go back to re-run analysis.</p> <p>Below you find a quick list of recommendations to make it easier for you to write useful analysis scripts. The recommendations are based on van Vliet (2020)[^1] and the MEG-BIDS guidelines[^2]. I recommend that you take a look at these when you have to write your own analysis scripts.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#comment-your-code","title":"Comment your code","text":"MATLABPython <p>In MATLAB you write comments with the percentage symbol <code>%</code>. Use this to write short explanations of what section or even single lines of code in your scripts do.</p> <p>For example:</p> <pre><code>    %% Calculate PSD: multitaper\n    cfg = [];\n    cfg.output    = 'pow';          % Return PSD\n    cfg.channel   = 'all';          % Calculate for all channels\n    cfg.method    = 'mtmfft';       % Use multitaper FFT\n    cfg.taper     = 'dpss';         % Multitapers based on Slepian sequences\n    cfg.tapsmofrq = 2;              % Smoothing +/- 2 Hz\n    cfg.foilim    = [1 95];         % Frequency 1 to 95 Hz\n    cfg.pad       = 'nextpow2';     % Padding option\n    psd_dpss = ft_freqanalysis(cfg, epochs);\n</code></pre> <p>In Python you write comments with the hash-tag symbol <code>#</code>. Use this to write short explanations of what section or even single lines of code in your scripts do.</p> <p>For example:</p> <pre><code>    # Define the frequency range for the analysis\n    config = {} # config dictionary\n    config['low_freq'] = 1 # low frequency\n    config['high_freq'] = 60 # high frequency\n    \"\"\"\n    You can also add longer segments of text that should not be treated as code, but as plain text like this.\n    \"\"\"\n</code></pre> <p>There are several reasons why you should comment your scripts. The first reason is that it makes it much easier to go back to your old scripts and know what they are supposed to do. What is self-evident when you first write your code might not be evident years later. The time you spent on writing comments in your code will come back later. The second reason for commenting your code is the usefulness if you are part of collaboration where you have to share data and scripts. What is self-evident for you might not be evident for other people. The third reason is that there is an increase in demand for sharing analysis scripts when publishing scientific articles, either for review purposes or demand by publishers that it has to be made available upon publication. Make it easier for the reviewers to understand what you are doing with your data. And finally, writing what the code is supposed to do helps you identify code that is not working correctly.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#install-packages","title":"Install packages","text":"Python <p>In python you need to import the modules and/or functions that you will use. Many are included by default, but you will need to install some additional modules. How to do this will not be covered in this tutorial. Make a habit of importing the libraries in the beginning of your script.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport mne\n</code></pre>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#use-section-breaks-when-testing-code","title":"Use section breaks when testing code","text":"<p>When writing code you often want to run only a small snip of code, e.g. when you test your code while scripting.</p> MATLABPython <p>If you only want to run parts of your script, you can mark code and select <code>run selection</code> or press <code>F9</code>. However, constantly marking code manually becomes annoying, really fast. Instead, use section breaks. You start a section with <code>%%</code> (two percentage signs). The line is commented, and you will notice that the section is highlighted in a yellowish colour. If you now press <code>ctrl+enter</code> you will run the code in the highlighted section.</p> <pre><code>%% Make a section\nx = 1:10;\ny = rand(size(x));\n\n%% Make a new section\nplot(x, y, 'or')\n</code></pre> <p>Constantly marking code manually becomes annoying, really fast. Instead, use section breaks. You start a section with <code># %%</code> (hash and two percentage signs). The line is commented, and you will notice that the section is grey (with defualt settings). If you now press ctrl+enter / cmd+enter you will run the code in the highlighted section. This is not a standard Python feature, but many IDEs and text editors support this feature. For example, in Jupyter notebooks, you can use this to run a cell at a time.</p> <pre><code># %%\nx = list(range(10))\ny = [random.random() for _ in x]\n\n# %%\nplt.plot(x, y, 'or')\n</code></pre>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#define-the-paths-and-toolboxes-at-the-beginning-of-the-script","title":"Define the paths and toolboxes at the beginning of the script","text":"MATLAB\u00a0Python <p>For this tutorial, you will use the toolbox FieldTrip to analyses MEG/EEG data. FieldTrip is written in MATLAB but is not a part of MATLAB. We, therefore, need to make sure that MATLAB has FieldTrip in its PATH definition to use the functions. The same applies if you use other toolboxes. This is simple: simply use the MATLAB function  <code>addpath( ... )</code> to add the path where you downloaded FieldTrip. If you have several versions of FieldTrip or have used other toolboxes before you run this script, it is also a good idea to restore the PATH with the function <code>resotredefaultpath</code></p> <p>The start of my script may look like this:</p> <pre><code>restoredefaultpath\naddpath('/home/mikkel/fieldtrip')       % Change to your path\nft_defaults\n</code></pre> <p>If you have several variables with the same names, it might also be good to add the following before the above code, to clear all figures and variables from your workspace. <pre><code>close all       % Close all open windows\nclear all       % Clear all variables from the workspace\n</code></pre> After this, we are ready to begin our script, and we will use the version of FieldTrip that we know we have at the given location.</p> <p>The start of my script may look like this with import of the necessary modules and definition of paths.</p> <pre><code>import mne\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nhome_path = '/Users/&lt;user&gt;/'\nproject_path = home_path + 'meeg_course_mne/'\n</code></pre>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#run-all-analysis-with-one-version-of-the-software","title":"Run all analysis with one version of the software","text":"<p>Toolboxes for data analysis gets updated regularly. FieldTrip, for example, is updated with a new version daily to keep the functionality up to date or to fix bugs in the code that users might have occurred. However, do not update FieldTrip daily! When you begin a project, make sure that all data is processed with the same version of the software that you use. If you need to update (which you sometimes need to do) make sure that your code is backwards compatible with the updated toolbox. If you need to update, better re-run everything.</p> <p>If you have many ongoing projects, it is useful to have several versions to make sure that you use the same versions for each project.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#one-script-does-one-data-processing-step","title":"One script does one data processing step","text":"<p>It might seem like a good idea to have one big script that you only have to run once to go from raw data to the finished result. It is not! It only makes it difficult to find bugs and errors. Instead, try to follow the principle:</p> <p>one script = one analysis step</p> <p>For example, one script that import raw data from disc, does the pre-processing, and then save the processed data to script. You can then easily name your scripts in the order they should run and call them from master script, e.g.:</p> \u00a0bash | MATLAB\u00a0bash | Python <pre><code>S01_import_data.m\nS02_run_ica.m\nS03_evoked_analysis.m\nS04_source_analysis.m\nS05_statistics.m\n</code></pre> <pre><code>python S01_import_data.py\npython S02_run_ica.py\npython S03_evoked_analysis.py\npython S04_source_analysis.py\npython S05_statistics.py\n</code></pre>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#save-intermediate-results","title":"Save intermediate results","text":"<p>Save data after each processing step. For example, each script above will save output to disk. Then the next step then reads the output from the previous step. Especially before and after steps that require manual intervention. This makes it easier to go back and redo individual steps without the need to re-run everything again.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#visualize-results-of-intermediate-processing-steps","title":"Visualize results of intermediate processing steps","text":"<p>Thought the tutorial you will be asked a lot to plot data and inspect data structures. This is not just to keep you occupied! It is good practice to visualize the outcome of each processing step (when you also would save the data). When you visualize the output of each processing step, it is easy to spot errors as they occur rather than only noticing them in the end results\u2014if you even notice them at all by then.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#use-consistent-filenames","title":"Use consistent filenames","text":"<p>Do not rename files each time you run the analysis. Use a consistent way to easy read what subject, session, and processing step the data belongs to. For example, output files from an analysis might look like this:</p> \u00a0bash | MATLAB\u00a0bash | Python <pre><code>sub-01-raw-tsss.fif\nsub-01-raw-downsampled.mat\nsub-01-epochs.mat\nsub-01-tfr.mat\n</code></pre> <pre><code>sub-01-raw-tsss.fif\nsub-01-raw-downsampled.fif\nsub-01-epochs.fif\nsub-01-tfr.fif\n</code></pre> <p>Note that each file has the id of the subject (<code>sub-01</code>) in all filenames and a string indicating what analysis step it belongs to.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#store-data-separate-by-subject-and-session","title":"Store data separate by subject and session","text":"<p>When you have data from multiple subjects resist the temptation to throw all data into one folder. Instead, create a project folder where you have one folder per subjects. And if you have more than one session per subject, you should then have separate sub-folders in the subject folder:</p> \u00a0bash | MATLAB\u00a0bash | Python <pre><code>/home/&lt;user&gt;/my_project/data/ ...\n    ./sub-01 ...\n        ./ses-01 ...\n            ./sub-01-ses-01-raw-tsss.fif\n            ./sub-01-ses-01-raw-downsampled.mat\n            ./sub-01-ses-01-epochs.mat\n            ./sub-01-ses-01-tfr.mat\n            ... etc.\n        ./ses-02 ...\n            ./sub-01-ses-02-raw-tsss.fif\n            ./sub-01-ses-02-raw-downsampled.mat\n            ./sub-01-ses-02-epochs.mat\n            ./sub-01-ses-02-tfr.mat\n            ... etc.\n    ./sub-02 ...\n    ... etc.\n</code></pre> <pre><code>/home/&lt;user&gt;/my_project/data/ ...\n    ./sub-01 ...\n        ./ses-01 ...\n            ./sub-01-ses-01-raw-tsss.fif\n            ./sub-01-ses-01-raw-downsampled.fif\n            ./sub-01-ses-01-epochs.fif\n            ./sub-01-ses-01-tfr.fif\n            ... etc.\n        ./ses-02 ...\n            ./sub-01-ses-02-raw-tsss.fif\n            ./sub-01-ses-02-raw-downsampled.fif\n            ./sub-01-ses-02-epochs.fif\n            ./sub-01-ses-02-tfr.fif\n            ... etc.\n    ./sub02 ...\n    ... etc.\n</code></pre> <p>When you keep this structure, it is  easy to set up subject specific paths to loop though when processing multiple subjects.</p> <p>In the tutorial data, you will find one subject called \"NatMEG_0177\" with one session called \"170424\" (the recording date; not the best session name). </p> <p>We can then setup subject and recording specific paths as below. The cell array <code>subjects_and_dates</code> can be expanded with more subjects and sessions when needed. You will see these lines of code several times throughout the tutorials.</p> \u00a0MATLAB\u00a0Python <pre><code>%% Define subjects and sessions\nsubjects_and_dates = ...\n             {\n                'NatMEG_0177/170424/'\n             };\n\nsubjects = ...\n            {\n                'NatMEG_0177'\n            };\n\nfilenames = {...\n        'tactile_stim_raw_tsss_mc.fif'\n        'tactile_stim_raw_tsss_mc-1.fif'\n        'tactile_stim_raw_tsss_mc-2.fif'\n            };\n</code></pre> <pre><code># %% Define subjects and sessions in lists\n\nsubjects_and_dates = [\n    'NatMEG_0177/170424/'\n    ]\n\nsubjects = [\n    'NatMEG_0177'\n    ]\n\nfilenames = [\n    'tactile_stim_raw_tsss_mc.fif',\n    'tactile_stim_raw_tsss_mc-1.fif',\n    'tactile_stim_raw_tsss_mc-2.fif'\n    ]\n</code></pre> <p>In your scripts, you can then easily loop though several subjects and run the same processing step on all subjects. You also make sure that you always read and save data to the correct folder by generating the paths within the loops, rather than specifying it manually in each script; e.g., like this:</p> \u00a0MATLAB\u00a0Python <pre><code>output_path = fullfile(meg_path, subjects_and_dates{1});\n</code></pre> <pre><code>from os import path\n\nmeg_path = path.join(project_path, 'MEG')\n\noutput_path = path.join(meg_path, subjects_and_dates[0])  # Note that Python starts counting at 0\n</code></pre>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#specify-paths-and-subject-names-once","title":"Specify paths and subject names once","text":"<p>When you run multiple scripts on several subjects and have to go back to redo steps of the analysis you can easily loose track of what files belong to which and when. You might end up with several versions of the same file processed with slightly different options. You want to make sure that when you go on to the next step, you read the correct files. If you change the filenames along the way, it is easy to loose track of which files that are new and old and you are prone to make critical errors</p> <p>The best way to avoid such errors is to specify subject id and filenames as few places as possible; ideally only once! This can be done by making a meta-script where you specify filenames and subject id (such as the code snippet above) that you then run in the beginning of each script.</p>"},{"location":"natmeg/server/script-tips/00_tips_for_writing_scripts/#ask-for-help","title":"Ask for help","text":"<p>If you encounter an error or problem that you do not know how to solve, there is a high likelihood that someone else have encountered the exact same problem before you. You might find that the answer has already been answered in an online forum or on one of the many MEG/EEG mailing list. Simply starting with googling the error message or the issue you are uncertain about often gives you the solution you need.</p> <p>There are also a lot of good resources for tips and tricks on MEG/EEG analysis. The MEG/EEG community is very open and helpful. Not only are the major analysis toolboxes open-source (which mean you can use it freely and even contribute yourself), they all have mailing lists that you can sign up for and ask for help from MEG/EEG scientists around the world. They tend to be quick to reply and friendly. Do not be afraid to ask for help!</p> <p>[^1]: van Vliet, M. (2020). Seven quick tips for analysis scripts in neuroimaging. PLOS Computational Biology, 16(3), e1007358. https://doi.org/10.1371/journal.pcbi.1007358van</p> <p>[^2]: The Brain Imaging Data Structure (BIDS) is an initiative to standardize how neuroimaging data is stored to allow easy sharing of data across research groups and sites. Originally BIDS was for sharing MRI data but has since been expanded for MEG (and by extension EEG). Niso, G., Gorgolewski, K. J., Bock, E., Brooks, T. L., Flandin, G., Gramfort, A., Henson, R. N., Jas, M., Litvak, V., T. Moreau, J., Oostenveld, R., Schoffelen, J.-M., Tadel, F., Wexler, J., &amp; Baillet, S. (2018). MEG-BIDS, the brain imaging data structure extended to magnetoencephalography. Scientific Data, 5(1).</p>"},{"location":"natmeg/server/script-tips/Consistent-filenames-in-loops/","title":"Consistent filenames in loops","text":"<p>Consistent filenames are key to data management. If you name your output files manually every time you save a file, you risk making errors. Do not rename files each time you run the analysis. Use a consistent way to read what subject, session, and processing step the data belongs to.</p> <p>In the top of a script (or better in a separate <code>config</code> script that you run at the beginning of every other script), you specify the base string of the filename. E.g.:</p> \u00a0FieldTrip\u00a0MNE-Python <pre><code>%% Base file names\nraw_fstring = '-proc-tsss-raw.fif';          % The raw data\nds_fstring  = '-proc-ds200-raw.mat';   % Downsampled and cleaned raw data\nepo_fstring = '-epochs.mat';            % Epoched data\n</code></pre> <pre><code># Base file names\nraw_fstring = '-proc-tsss-raw.fif'          # The raw data\nds_fstring  = '-proc-ds200-raw.fif'         # Downsampled and cleaned raw data\nepo_fstring = '-epo.fif'                   # Epoched data\n</code></pre> <p>Then in the loop/function where you run the data processing, you specify the subject-specific input- and output filenames. </p> \u00a0FieldTrip\u00a0MNE-Python <p>This is an example of how to do in a Matlab loop:</p> <pre><code>%% Run loop...\nfor ss = 1:length(all_subjects)\n   subj = all_subjects{ss};\n\n   % Define filenames\n   infname  = fullfile(my_data_path, subject_folder_path, [subj, ds_fstring]);    % Read '-raw-downsampled.mat'\n   outfname = fullfile(my_data_path, subject_folder_path, [subj, epo_fstring]);   % Save '-epochs.mat'\n\n   % Load data\n   load(infname)\n\n   % Run your process\n   ...\n\n   % Save\n   save(outfname, mydata)\nend\n</code></pre> <p>This is an example of how to do in a Python loop:</p> <pre><code># Run loop...\nfor ss in all_subjects:\n   subj = ss\n\n   # Define filenames\n   infname  = os.path.join(my_data_path, subject_folder_path, subj + ds_fstring)    # Read '-proc-ds200-raw.fif'\n   outfname = os.path.join(my_data_path, subject_folder_path, subj + epo_fstring)   # Save '-epo.fif'\n\n   # Load data\n   raw = mne.io.read_raw_fif(infname) # Read data\n\n   # Run your process\n   epochs = mne.Epochs(raw)\n\n   # Save\n   epochs.save(outfname) # Save epochs\n</code></pre> <p>Example with multiple sessions and subjects:</p> \u00a0FieldTrip\u00a0MNE-Python <pre><code>%% Run loop...\nfor ss = 1:length(all_subjects)\n   for jj = 1:length(all_sessions)\n\n      subj = all_subjects{ss};\n      session = all_sessions{jj}\n\n      % Define filenames\n      infname  = fullfile(my_data_path, subject_folder_path, [subj,'-',session, ds_fstring]);    % Read '-raw-downsampled.mat'\n      outfname = fullfile(my_data_path, subject_folder_path, [subj,'-',session, epo_fstring]);   % Save '-epochs.mat'\n\n      % Load data\n      load(infname)\n\n      % Run your process\n      ...\n\n      % Save\n      save(outfname, mydata)\n   end\nend\n</code></pre> <pre><code># Run loop...\nfor ss in all_subjects:\n   for jj in all_sessions:\n\n      subj = ss\n      session = jj\n\n      # Define filenames\n      infname  = os.path.join(my_data_path, subject_folder_path, subj + '-' + session + ds_fstring)    # Raw name '-proc-ds200-raw.fif'\n      outfname = os.path.join(my_data_path, subject_folder_path, subj + '-' + session + epo_fstring)   # Save '-epo.fif'\n\n      # Load data\n      raw = mne.io.read_raw_fif(infname) # Read data\n\n      # Run your process\n      epochs = mne.Epochs(raw)\n\n      # Save\n      epochs.save(outfname) # Save epochs\n</code></pre>"},{"location":"natmeg/server/script-tips/Estimate-data-rank/","title":"Estimate rank","text":"<p>It is important to know the rank of the data for several common MEG processing steps, such as source reconstruction and data whitening. This is particularly important when you have processed your data with MaxFilter which reduce the rank of the data drastically.</p> <p>After MaxFilter the rank is usually around 72. The rank is reduced further if you remove, e.g. ICA components during pre-processing.</p> FieldTripMNE-Python","tags":["MATLAB","FieldTrip","MNE-Python"]},{"location":"natmeg/server/script-tips/Estimate-data-rank/#in-fieldtrip","title":"In FieldTrip","text":"<p>In <code>ft_sourceanalysis</code> you can specify the rank as \"kappa\" in the <code>cfg</code>. Calculate Kappa like this:</p> <p><pre><code>%% Calculate Kappa\ncfg = [];\ncfg.covariance          = 'yes';\ncfg.covariancewindow    = 'all';\ncfg.channel             = 'MEG';\ndata_cov = ft_timelockanalysis(cfg, epo);\n\n[u,s,v] = svd(data_cov.cov);\nd       = -diff(log10(diag(s)));\nd       = d./std(d);\nkappa   = find(d&gt;5,1,'first');\nfprintf('Kappa = %i\\n', kappa)\n</code></pre> Taken from this tutorial: https://www.fieldtriptoolbox.org/workshop/paris2019/handson_sourceanalysis/</p>","tags":["MATLAB","FieldTrip","MNE-Python"]},{"location":"natmeg/server/script-tips/Estimate-data-rank/#in-mne-python","title":"In MNE-Python","text":"<p>MNE-Python has a build-in function to estimate rank of the data called <code>mne.compute_rank</code>. See the full documentation here: https://mne.tools/stable/generated/mne.compute_rank.html?highlight=rank#mne.compute_rank</p> <p>Some (!) MNE-Python functions will estimate the rank on the fly. but sometimes it will assume the rank is equal to the number of channels rather than calculate it. It is safer to use <code>mne.compute_rank</code> and make sure the rank is in the ballpark of what you expect.</p>","tags":["MATLAB","FieldTrip","MNE-Python"]},{"location":"natmeg/server/script-tips/How-to-find-all-raw-files-that-belongs-to-the-same-condition/","title":"How to find all raw files that belongs to the same condition","text":"<p>For example, if you have split files called \"taskA.fif\" and \"taskA-1.fif\" that not necessarily is equal between subjects, but want to read all files with the name \"taskA\" in the filename.</p> <p>Use this function: https://github.com/natmegsweden/NatMEG_Wiki/blob/main/example_scripts/find_files.m</p> <p>Copy/paste it to your file path and then use it like this: </p> MATLABPython <pre><code>% Find files\nsubpath = '/some/path/with/all/data'   % Replace with real data path\nfile_list = find_file(subpath, {'taskA', 'tsss'}) % Find all files with \"taskA\" and \"tsss\" in the filename\n\n% Load the data (one way to do it)\nfor ff = 1:file_list\n   data{ff} = load(file_list{ff})\nend\n</code></pre> <pre><code># Find files\nfrom glob import glob\n\nsubpath = '/some/path/with/all/data'   # Replace with real data path\nfile_list = glob(f'{subpath}*taskA*tsss*') # Find all files with \"taskA\" and \"tsss\" in the filename\n</code></pre>","tags":["MATLAB","FieldTrip","Python","MNE-Python"]},{"location":"natmeg/server/script-tips/Ignore-trials-split-between-fif-files/","title":"Ignore split trials between fif files","text":"<pre><code>% Define trials\ncfg                     = [];   \ncfg.dataset             = %\ncfg.trialdef.prestim    = %\ncfg.trialdef.poststim   = %\ncfg.trialdef.eventvalue = %\ncfg.trialfun            = %\ncfg                     = ft_definetrial(cfg);\n\n%Remove trials from cfg.trl that have negative sample index for trial start\nnotneg = cfg.trl(:,1) &gt;= 0\ncfg.trl = cfg.trl(notneg,:)\n\n%Remove trials from cfg.trl that have higher sample index than exist in file\ntoolate = cfg.trl(:,2) &lt; max([cfg.event.sample]);\ncfg.trl = cfg.trl(toolate,:)\n\n% Preprocess\ncfg.channel = %\n\ndata = ft_preprocessing(cfg);\n</code></pre>","tags":["MATLAB","FieldTrip"]},{"location":"natmeg/server/script-tips/Loop-through-figures-for-inspection/","title":"Loop through figures for inspection","text":"<p>An example of how to use <code>uiwait()</code> in a loop to inspect some summary figure per subject. Mind that <code>load()</code> may be slow with large files. <pre><code>for i = 1:length(sub_date.ID);\n   %from ft_read_headshape(MEGfile);\n   load(['../mat_data/MRI_mat/' 'ID' sub_date.ID{i} '_headshape']);\n\n   %from ft_read_sens(MEGfile);\n   load(['../mat_data/MRI_mat/' 'ID' sub_date.ID{i} '_sensshape']);\n\n    %from ft_prepare_headmodel(cfg, mesh_brain);\n    load(['../mat_data/MRI_mat/' 'ID' sub_date.ID{i} '_MEG_headmodel']);\n\n    %Final plot - aligned MEG\n    fig = figure('Position', [800 300 900 900]); %[Left Bottom Width Height]\n    hold on;\n    ft_plot_sens(sensshape)\n    ft_plot_headshape(headshape)\n    ft_plot_headmodel(headmodel_meg)\n    ft_plot_axes([], 'unit', 'cm');\n\n    title(['SUBJECT: ' sub_date.ID{i}]);\n\n    %Pause loop until figure is closed\n    uiwait(fig);\n\nend\n</code></pre></p> <p>This loop presents this figure:</p> <p></p>","tags":["MATLAB","FieldTrip"]},{"location":"natmeg/server/script-tips/MATLAB-paths/","title":"MATLAB paths","text":"<p>When adding paths to your MATLAB pathdef, you should save the pathdef in a local folder E.g., if you want to add a toolbox like FieldTrip permanently to your path.</p> <p>When you install MATLAB, it should create a folder in your home directory called <code>~/matlab</code>. Save the pathdef as <code>pathdef.m</code> in this folder.</p> <p>Using the Set Path tool from the menu in the MATLAB GUI might give an error if you add paths and try to save from the GUI tool. A popup will appear that asks if you want to save in another location. Click \"yes\" and save as <code>~/matlab/pathdef.m</code>.</p> <p>The reason for the error is that MATLAB tries to save the pathdef in the shared MATLAB folder in <code>/opt</code>.</p>","tags":["MATLAB","FieldTrip"]},{"location":"natmeg/server/script-tips/Morphing%2C-normalization%2C-and-transforming-source-spaces-and-coordinate-systems-%28MNE%29/","title":"Morphing, normalization, and transforming source spaces and coordinate systems (MNE)","text":"<p>This page is various quick guides to normalization of source estimates with MNE-Python and working with group level data. </p>","tags":["MNE-Python","source analysis","normalization"]},{"location":"natmeg/server/script-tips/Morphing%2C-normalization%2C-and-transforming-source-spaces-and-coordinate-systems-%28MNE%29/#read-labels-from-standard-atlas","title":"Read labels from standard atlas","text":"<p>In MNE you transform atlas from fsaverage to subject anatomy. The advantage is that Freesurfer comes with several standard atlases that is already transformed to individual anatomy as part of the standard Freesurfer pipeline. Just read the labels:</p> MNE-Python <pre><code>labels = mne.read_labels_from_annot(subj,\n                                    parc=atlas,\n                                    subjects_dir=subjects_dir,\n                                    hemi=hemi)\n</code></pre> <p>If you want to use atlases that are not part of Freesurfer as a default, you need to add them to Freesurfer first: see this example script.</p>","tags":["MNE-Python","source analysis","normalization"]},{"location":"natmeg/server/script-tips/Morphing%2C-normalization%2C-and-transforming-source-spaces-and-coordinate-systems-%28MNE%29/#transform-source-space-to-common-space","title":"Transform source space to common space.","text":"<p>With MNE you usually transform to the Freesurfer template \"fsaverage\" for group analysis. To transform from subject source space to common space, MNE make use of a \"morph\" class that once constructed is applied to the source estmates in downstream processing steps. For more information on the \"morph\" class, see the MNE documentation.</p> <p>Follow the MNE tutorials to see how to create and apply morphing in MNE:</p> <ul> <li>Morph surface source space (the most common in MNE)</li> <li>Morph volume source space</li> </ul>","tags":["MNE-Python","source analysis","normalization"]},{"location":"natmeg/server/script-tips/Plot-head-channel-alignment-in-FieldTrip/","title":"Plot head-channel alignment in FieldTrip","text":"<p>Recommendation: always plot the head models together with the head points and sensors as a sanity check that the process went well.</p> <p>Easy code snippet you can copy paste to your script:</p> \u00a0FieldTrip <pre><code>%% Plot all aligned: MEG\nfigure; hold on\nft_plot_sens(grad)\nft_plot_headshape(headshape)\nft_plot_headmodel(headmodel)\nft_plot_axes([], 'unit', 'cm');\n</code></pre> <p></p>"},{"location":"natmeg/server/script-tips/Read-non-MaxFiltered-data/","title":"Read non MaxFiltered data","text":"<p>As a general rule, you should always run MaxFilter on data recorded in the NatMEG lab, as the internal active shielding is always on during recordings due to the large number of magnetic noise generators around the KI campus.</p> <p>For some applications, it might however be useful to read non-MaxFiltered data, e.g. when testing trigger codes in pilot data. To protect you from running analysis on non-MaxFiltered data, most analysis toolboxes will give you an error if you try to read non-MaxFiltered data.</p> <p>Here is a guide on how to circumvent the error.</p> FieldTripMNE-Python"},{"location":"natmeg/server/script-tips/Read-non-MaxFiltered-data/#fieldtrip","title":"FieldTrip","text":"<p>You can read non-MaxFiltered data in FieldTrip with the low-level function <code>ft_read_data</code>. You have to specify the keyword <code>'checkmaxfilter'</code> as <code>'no'</code> (the default is <code>'yes'</code>, i.e. to ensure that MaxFilter has run). <pre><code>%% Read data header\nhdr = ft_read_header(\"your_filename\", 'checkmaxfilter', 'no');\n\n%% Read raw non-MaxFiltered data\ndat = ft_read_data(\"your_filename\", 'checkmaxfilter', 'no');\n</code></pre></p> <p>This will, however, read the data as a MATLAB matrix rather than as a FieldTrip data structure that you get when importing data with <code>ft_preprocessing</code>. If you are looking for specific channels, e.g. only the trigger channels, it is a good idea to specify that you only want to read the specific channels. Otherwise, it will read all channels no matter what data type it is. For more information see the documentation for ft_read_data</p>"},{"location":"natmeg/server/script-tips/Read-non-MaxFiltered-data/#mne-python","title":"MNE-Python","text":"<p>In MNE-Python you can read non-MaxFiltered data the same way as you read raw data with <code>mne.io.read_raw_fif</code>, with the additional argument <code>allow_maxshield</code> set to <code>False</code>:</p> <p><pre><code>raw_nomax = mne.io.read_raw_fif(\"your_filename\", allow_maxshield=False)  # Read non-MaxFiltered data\n</code></pre> For more information see the documentation for read_raw_fif</p>"},{"location":"natmeg/server/script-tips/Read-split-files-into-FieldTrip/","title":"Read split files into FieldTrip","text":"<p>Sometimes data recordings are split into several files, either because the recording was started and stopped during data acquisition, or because the 2 GB maximum file size of FIF means that long recording sessions are split over several files. When you read data into FieldTrip, you then need to merge the split files into one data file for further processing and correct the samples so that the files align. There are several ways you can achieve this.</p>","tags":["FieldTrip","MEG","preprocessing"]},{"location":"natmeg/server/script-tips/Read-split-files-into-FieldTrip/#option-a-read-files-from-cell-array-the-best-solution","title":"Option A: Read files from cell-array (the best solution)","text":"<p>This might not work! Tested 2021-05-25 and it worked for filenames listed in a cell array</p> <p>Specify the filenames of split-files in a cell array (or use find_files):</p> <p><pre><code>fnames = {'file1.fif', 'file2,fif', 'file3.fif'}\n</code></pre> Then specify <code>cfg.dataset</code> as the array of filenames. Example:</p> <pre><code>% Define trials\ncfg = [];\ncfg.dataset             = fnames ;  % This is the cell-array\ncfg.trialdef.prestim    = 1;        % seconds before trigger\ncfg.trialdef.poststim   = 1;        % seconds after trigger\ncfg.trialdef.eventtype  = 'STI101';\ncfg.trialdef.eventvalue = [257, 258, 260];\ncfg.trialfun            = 'ft_trialfun_neuromagSTI016fix';\n\ncfg = ft_definetrial(cfg);\n\n% preprocessing\ncfg.demean     = 'yes';\ncfg.lpfilter   = 'yes';\ncfg.lpfreq     = 100;\ncfg.hpfilter   = 'no';\ncfg.dftfilter  = 'no';\ncfg.allowoverlap = 'yes';\ncfg.channel    = {'MEG', 'ECG', 'EOG'};\n\nepochs = ft_preprocessing(cfg);\n</code></pre>","tags":["FieldTrip","MEG","preprocessing"]},{"location":"natmeg/server/script-tips/Read-split-files-into-FieldTrip/#option-b-read-all-files-concatenate-and-then-create-trials","title":"Option B: Read all files, concatenate, and then create trials","text":"<p>Manually correct sample info by reading the header and events with <code>ft_read_header</code> and <code>ft_read_event</code>. </p> <p>First, specify the filenames:</p> <pre><code>infiles = {\n    fullfile(raw_path,'data_tsss_mc.fif'),\n    fullfile(raw_path,'data-1_tsss_mc.fif')\n    };\n</code></pre> <p>Then read events and headers. This code assumes that the trigger channel is <code>STI101</code>.</p> <pre><code>%% Read events\nfor ii = 1:length(infiles)\n    hdrs{ii} = ft_read_header(infiles{ii});\n    idx = find(not(cellfun('isempty',strfind(hdrs{ii}.label,'STI101'))));\n    eve{ii} = ft_read_event(infiles{ii},'chanindx',idx);\nend\n</code></pre> <p>Now, correct the sample value by adding the number of samples in the previous file to the sample index of the next file.</p> <pre><code>sam1 = [eve{1}.sample]; \nsam2 = [eve{2}.sample]+hdrs{1}.nSamples;\nallsam = [sam1, sam2];\neve2 = appendstruct(eve{1}, eve{2});\n</code></pre> <p>Plot to see that it align, as it should.</p> <pre><code>figure; hold on\nplot(allsam, [eve2.value], 'k');\nscatter(allsam, [eve2.value], 'r')\n</code></pre> <p>In the next step, I did the selection based on the trigger values in the different conditions, like this: <code>rsp_taskA = [eve2.value] == 2</code>. I did this for all the different conditions; in this case, three different conditions.</p> <p>Then create the <code>trl</code> structure manually. Specify how much time you would like before (<code>prestim</code>) and after (<code>poststim</code>). In contrast, to <code>ft_define_trial</code>, this should be specified in number of samples rather than seconds.</p> <pre><code>sam_taskA = allsam(rsp_taskA);\nsam_taskB = allsam(rsp_taskB);\nsam_taskC = allsam(rsp_taskC);\nsam_cmb = [sam_taskA, sam_taskB, sam_taskC]';       % Merge trigger samples\ncon_cmb = ones(size(rsp_taskA)),ones(size(rsp_taskB))*2,ones(size(rsp_taskC))*3]';  % Create label for conditions for bookkeeping.\n\n% Create the trial structure.\nprestim = -1000;        % -1000 samples\npoststim = 1000;        % +1000 samples\ntrl = [sam_cmb+prestim, sam_cmb+poststim, repmat(prestim,size(sam_cmb)), con_cmb];\n</code></pre> <p>Then you can read in the MEG data split-files as usual and do whatever initial pre-processing you want to do with <code>ft_preprocessing</code>. Then use <code>ft_redefinetrial</code> to chop into epochs based on your previously defined <code>trl</code> structure.</p> <pre><code>%% Read all datafiles\nfor ii = 1:length(infiles)\n    cfg = [];\n    cfg.channel         = 'MEG';\n    cfg.dataset         = infiles{ii};\n    split_meg{ii} = ft_preprocessing(cfg);\nend\n\n%% Merge split files\ncfg = [];\ncfg.keepsampleinfo='no';\nraw_meg = ft_appenddata(cfg,split_meg{:});\n\n%% Make epochs\ncfg = [];\ncfg.trl = trl;\nepo_meg = ft_redefinetrial(cfg, raw_meg);\n</code></pre>","tags":["FieldTrip","MEG","preprocessing"]},{"location":"natmeg/server/script-tips/Read-split-files-into-FieldTrip/#option-c-make-epochs-immediately-then-merge-split-files","title":"Option C: Make epochs immediately, then merge split files","text":"<p>See here</p>","tags":["FieldTrip","MEG","preprocessing"]},{"location":"natmeg/server/script-tips/Select-limited-number-of-trials-in-FieldTrip/","title":"Select a limited number of trials in FieldTrip","text":"<p>Select a subsample of trials from a dataset. E.g. if you ant to test the number of trials needed to see a response in pilot data, or compare first vs last trials.</p> <p>In this example, I select the number of trials in steps of 10 to 100 in a loop. There are three examples: 1. Select the N first trials. 2. Select the N last trials. 3. Select N trials at random (to avoid bias).</p> <p>I start with cleaned and downsampled data; all preprocessing is already done on this data.</p> <pre><code>%% Number of trials in data\nnTrials = length(cleaned_downsampled_data.trial); % is 100 in this example\n\n%% Select in steps\nii=0; %\nfor nn = 10:10:nTrials \n    ii = ii+1;\n\n    % A: select nn first trials\n    cfg = [];\n    cfg.trials = 1:nn;\n    dat_nFirst{ii} = ft_selectdata(cfg, cleaned_downsampled_data);\n\n    % B: select nn last trials\n    cfg = [];\n    cfg.trials = nTrials-nn:nTrials;\n    dat_nLast{ii} = ft_selectdata(cfg, cleaned_downsampled_data);\n\n    % C: Select nn random trials\n    idx = zeros(nTrials, 1);\n    idx(1:nn) = 1;\n    idx = logical(idx(randperm(length(idx))));\n\n    cfg = [];\n    cfg.trials = idx;\n    dat_nRand{ii} = ft_selectdata(cfg, cleaned_downsampled_data);\n\nend\n</code></pre> <p>Then average to get evoked responses. Here I only run on the data with N first trials, but you can do the same for the other ways of selecting trials.</p> <pre><code>%% Average\nfor ii = 1:length(dat_nFirst)\n    cfg = [];\n    dat_nFirst_avg{ii} = ft_timelockanalysis(cfg, dat_nFirst{ii});\nend\n</code></pre> <p>Finally plot for comparison.</p> <pre><code>%% Plot for comparison\ncfg = [];\ncfg.layout = 'neuromag306mag.lay';\nft_multiplotER(cfg, dat_nRand{:})\nlegend(strsplit(num2str(10:10:nTrials)))\n</code></pre>","tags":["FieldTrip","scripting"]},{"location":"natmeg/server/set-up-connection/01_Connect-to-Archive/","title":"Connect to Archive","text":"<p>This is a guide on how to connect to NatMEG's archive server to access your data. To connect, you need a username and password for the server.</p> <p>There are two options for connecting:</p> <ol> <li>Using a FTP app like FileZilla client</li> <li>Using command line  </li> </ol>"},{"location":"natmeg/server/set-up-connection/01_Connect-to-Archive/#1-using-filezilla","title":"1. Using FileZilla","text":"<p>Download the FileZilla client (https://filezilla-project.org) for the OS you are using and install it.</p> <p>Connect to the sever with hostname: <code>archive.natmeg.se</code>, port: <code>22</code>, your username and password. You can now copy files from the server to your local machine.</p>"},{"location":"natmeg/server/set-up-connection/01_Connect-to-Archive/#2-using-command-line","title":"2. Using command line","text":"<p>Connect to the server with <code>sftp username@archive.natmeg.se</code> and enter your password when prompted. </p> <p>You can view the files and folders in your current remote (i.e., on the server) directory with <code>ls</code>, view the current remote directory with <code>pwd</code>, and change it with <code>cd REMOTEPATH</code>. To do the same for your local directory (i.e., on your computer) use <code>lls</code>, <code>lpwd</code> and <code>lcd LOCALPATH</code>, respectively. You can download files with <code>get REMOTE [LOCALPATH]</code>. REMOTE can be a file or folder. If you want to download an entire folder use <code>get -R REMOTE [LOCALPATH]</code>. If you do not specify LOCALPATH the downloaded files will go to your current local directory.</p>"},{"location":"natmeg/server/set-up-connection/01_Connect-to-Archive/#known-connection-issues","title":"Known connection issues","text":"<p>You should not need to be connected to KI via VPN, but you are not able to connect via eduroam. Use another secure network.</p> <p>If you get the following error: <code>no matching host key type found. Their offer: ssh-rsa,ssh-dss</code></p> <p>Add the following lines to your <code>.ssh/config</code>-file:</p> <pre><code>HostKeyAlgorithms ssh-rsa\nPubkeyAcceptedKeyTypes ssh-rsa\n</code></pre>"},{"location":"natmeg/server/set-up-connection/02_Connect-to-Storage/","title":"Connect to Storage","text":"<p>This is a guide on how to connect to NatMEG's storage server to access your home folder and shared data. To connect, you need a username and password for the server.</p> <p>There are three options:</p> <ol> <li>Using an SFTP app like FileZilla or Beyond-compare </li> <li>Using SFTP from command line</li> <li>Using a samba file share </li> </ol>"},{"location":"natmeg/server/set-up-connection/02_Connect-to-Storage/#1-using-filezilla","title":"1. Using FileZilla","text":"<p>Download the FileZilla client (https://filezilla-project.org) for the OS you are using and install it.</p> <p>Connect to the sever with hostname: <code>storage02.natmeg.se</code>, port: <code>22</code>, your username and password. You can now copy files from the server to your local machine.</p>"},{"location":"natmeg/server/set-up-connection/02_Connect-to-Storage/#2-using-command-line","title":"2. Using command line","text":"<p>Connect to the server with <code>sftp username@storage02.natmeg.se</code> and enter your password when prompted. </p> <p>You can view the files and folders in your current remote (i.e., on the server) directory with <code>ls</code>, view the current remote directory with <code>pwd</code>, and change it with <code>cd REMOTEPATH</code>. To do the same for your local directory (i.e., on your computer) use <code>lls</code>, <code>lpwd</code> and <code>lcd LOCALPATH</code>, respectively. You can download files with <code>get REMOTE [LOCALPATH]</code>. REMOTE can be a file or folder. If you want to download an entire folder use <code>get -R REMOTE [LOCALPATH]</code>. If you do not specify LOCALPATH the downloaded files will go to your current local directory.</p>"},{"location":"natmeg/server/set-up-connection/02_Connect-to-Storage/#3-using-a-samba-file-share","title":"3. Using a samba file share","text":"<p>Using samba you can map your home and shared folders from the server to your local computer. You need additional access from the core group for this so start by contacting them.  Once you have samba access you can map a network drive with \"\\storage02.natmeg.se\\username\" (Windows) or \"smb://username@storage02.natmeg.se/\" and your samba credentials . </p>"},{"location":"natmeg/server/set-up-connection/02_Connect-to-Storage/#known-connection-issues","title":"Known connection issues","text":"<p>You need to be connected to the KI network via VPN.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/","title":"Connect to Compute","text":"<p>This is a guide on how to connect to NatMEG's Compute server for data analysis. To connect, you need a username and password for the server.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#general-principles","title":"General principles","text":"<ul> <li>We share the CPU and RAM so be aware of others when running multiple heavy processes.</li> <li>Avoid machine learning (ML). Since much ML processing works better with GPU this is not suitable for the server with very limited GPU.</li> <li>Avoid web browsing. As the server is not built for graphics, using Firefox consumes a lot of resources.</li> </ul> <p>Connecting to Compute has three steps:</p> <ol> <li>Connect to KI's network (if working by remote)</li> <li>Connect to compute with username and password.</li> <li>Open VNC viewer (TurboVNC is recommended)</li> <li>Optionally, you can connect via remote-ssh using Visual Studio Code (note that you will not be able to view interactive plots)</li> </ol> <p>The first time you connect you need to start a VNC server and configure your settings. The guide below explains the steps in detail.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#connect-to-natmegs-servers-from-outside-kis-network","title":"Connect to NatMEG\u2019s servers from outside KI's network","text":"<p>To connect to the NatMEG servers, you need to be on KI\u2019s network. If you are working on KI you can go to step 2: Connect to Compute with VNC. If not, you first need to set up a VPN connection to KI.</p> <p>To remote connect to KI's network, you need to have a KI account and username and then connect via KI's VPN service and the PointSharp app.</p> <p>If you do not have a VPN connection to KI, you need to set this up first. This includes downloading a VPN app Cisco Anyconnect and a verification app PointSharp. If you do not have the PointSharp app already, you need to contact KI's IT support (fixit@ki.se) to get a verification code the first time you connect. You can find the official guide here: https://staff.ki.se/vpn-service.</p> <p>When you are on KI's network\u2014either by remote or physically present at KI\u2014you can connect to Compute.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#setup-a-connection-to-compute","title":"Setup a connection to Compute","text":"<p>If you have not connected to Compute before, you will need to do an initial setup.</p> \u00a0Windows\u00a0MacOS/Linux <p>If working on a Windows computer, you first need an SSH client (e.g. PuTTy if on PC www.putty.org) and a VNC client (e.g. SSVNC, https://sourceforge.net/projects/ssvnc/files/ssvnc/1.0.29 or TightVNC Viewer, www.tightvnc.com/download.php). Linux and Mac computers already support SSH. Mac computers also come with a VNC client already installed.</p> <p>Once you have this in place, you can connect to compute and create a VNC session.</p> <p>Follow this procedure to set up the VNC connection to Compute:</p> <p>Open PuTTy. On the main tab enter \u201cHost Name (or IP address)\u201d as <code>compute.natmeg.se</code>. Use port <code>22</code> (typically the default), as shown below:</p> <p></p> <p>Click open. A terminal will pop up. Enter your username and password. You are now connected to Compute.</p> <p>Open a new terminal window and type: <code>ssh username@compute.natmeg.se</code>, where <code>username</code>is your personal username. Enter your password when asked. You are now connected to Compute.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#create-a-new-vnc-session","title":"Create a new VNC session","text":"<p>When connected to \u201ccompute\u201d, you must create a new VNC session (not to confuse with VPN). You do this in the terminal:</p> <ol> <li>Type: <code>vncserver</code> to open a new VNC server. If you want the remote session window to be of a certain size, add a geometry flag, e.g. <code>vncserver \u2013geometry 2560x1340</code>. Choose a geometry, i.e. screen resolution, that works with your own monitor.</li> <li>Note the session number you get in return and use this to connect to your VNC session later.</li> </ol> <p></p> <ol> <li>Type <code>vncserver \u2013list</code> to see your active VNC sessions. Type <code>vncserver \u2013kill :N</code> to close a server, where <code>N</code> is the server's number, e.g. <code>compute:20</code> in the picture above. For example, if you want to change the geometry of the window later on.</li> </ol> <p>The first time you create a VNC server you should be asked to create a password for the VNC sessions. Note that this password is only for the VNC and is different from the password you need to gain access to Compute. If you are not explicitly asked to create a password, type <code>vncpasswd</code> in the terminal to add/edit your VNC password. Please create a secure password. You will use the password every time you open the VNC window.</p> <p>Mac: Exit the ssh session with <code>exit</code>.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#configure-connection-for-future-connection","title":"Configure connection for future connection.","text":"WindowsMacOS/Linux <p>Open a new PuTTY. Go to the tab  Connection -&gt; SSH -&gt; Tunnels. Add \u201cSource port\u201d as <code>59XX</code> where XX is the number of your VNC server, e.g. 5902. Add \u201cDestination\u201d as <code>localhost:59XX</code> where XX is the number of your VNC server (see picture below). Click \u201cAdd\u201d.</p> <p></p> <p>Go to the tab Connection -&gt; SSH -&gt; X11. Make sure \u201cEnable X11 forwarding\u201d is checked. Add \u201cX display location\u201d <code>localhost:0</code>.</p> <p></p> <p>Before proceeding, go back to the first tab and save the settings.</p> <p>Open a new terminal and reconnect to Compute with SSH with the following command: <code>ssh username@compute.natmeg.se -L 59XX:localhost:59XX</code> where you replace <code>username</code> with your personal username and <code>XX</code> is your VNC session number. If you get a one-digit VNC id back, e.g. 4, your id is 04 (so you get <code>:5904</code>).</p> <p>If you experience a black screen with an error when you connect, see here.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#connect-to-compute-with-vnc","title":"Connect to compute with VNC","text":"<p>Once the setup is complete, you connect to the servers using a VNC client.</p> <p>We recommend TurboVNC as it is free, open source and you are able to adjust screen resolution and copy-paste between the VNC window and your local computer.</p> <p></p> Windows\u00a0MacOS/Linux <ol> <li>Open PuTTy. If you saved the settings (as instructed above), click \u201cOpen\u201d. Then enter your username and password. </li> <li>When connected to the server, open your VNC client. Then enter your username and password. <p>If you set X11 forwarding, as described above, connect to \u201cRemote Host\u201d <code>localhost:0</code>. If not, connect using your username + the IP address to compute (130.229.40.51) + an index (59) + your VNC session number. If you get a one-digit VNC session, e.g. 4, your id is 04 (so you get <code>:5904</code>).</p> </li> </ol> <ol> <li>Open a terminal and connect with SSH to Compute with the following command: <code>ssh username@compute.natmeg.se -L 59XX:localhost:59XX</code> where you replace <code>username</code> with your personal username and <code>XX</code> is your VNC session number</li> <li>When connected to the server, open your VNC client. Then enter your username and password.</li> </ol> <p>Or if you don't want to use external software, in the Mac menu \"Go\" and click \"Connect to Server...\". Enter the address as <code>vnc://localhost:59XX</code> where <code>XX</code> is your VNC session number.</p>"},{"location":"natmeg/server/set-up-connection/03_Connect-to-Compute/#close-vnc-server","title":"Close VNC server","text":"<p>You can leave your VNC session running between uses and reuse the same session number. However, if you know that you will not use Compute resources in a while (e.g. over a vacation or similar), we appreciate if you close your open VNC servers. </p> <p>To close a VNC server, connect to Compute and in the terminal write <code>vncserver -kill :XX</code> where <code>XX</code> is your VNC session number.</p>"},{"location":"natmeg/server/set-up-connection/04_Close-VNC-connection/","title":"Close VNC connection","text":""},{"location":"natmeg/server/set-up-connection/04_Close-VNC-connection/#close-vnc-connection-normal","title":"Close VNC connection (normal)","text":"<p>Type <code>vncserver \u2013kill :N</code> to close a server, where <code>N</code> is the server's number, e.g. <code>compute:20</code> in the picture below.</p> <p></p>"},{"location":"natmeg/server/set-up-connection/04_Close-VNC-connection/#close-vnc-connection-errors","title":"Close VNC connection (errors)","text":"<p>Sometimes, for some reason, it is not possible to use <code>vncserver \u2013kill</code> to shut down the VNC connection. You might get an error like this:</p> <pre><code>user@compute ~]$     /usr/bin/vncserver -kill :6 \nKilling Xvnc process ID 20730\nXvnc seems to be deadlocked.  Kill the process manually and then re-run\n    /usr/bin/vncserver -kill :6\nto clean up the socket files.\n</code></pre> <p>In that case, you must killer the VNC process manually. To do this find the process id (PID) of the VNC process. You can see the PID by calling <code>vncserver -list</code>, which list your active VNC sessions, the session number, and the corresponding PID. To kill the process, write the following in your terminal:</p> <p><pre><code>kill -9 PID\n</code></pre> Where \"PID\" is the PID number.</p>"},{"location":"natmeg/server/troubleshooting/Alt-key-does-not-work-from-vncserver-window/","title":"Problem: Alt-key does not work on Mac-keyboard","text":""},{"location":"natmeg/server/troubleshooting/Alt-key-does-not-work-from-vncserver-window/#solution","title":"Solution:","text":"<p>Disclaimer: this may only be a temporary fix, and a Mac-only problem</p> <ol> <li>Identify the keycode of your alt-key: <code>xev -event keyboard</code> press Alt. This will probably show double events, including Shift_L (probably keycode 50), check the keycode for the other events.</li> </ol> <p></p> <ol> <li>run <code>xmodmap -e \"keycode your_keycode = Alt_L\"</code></li> <li>to make it automatic when you start your vncserver add it to your .bashrc</li> </ol> <p>Example:</p> <p>For some reason I had to change it twice, thus adding the following lines to .bashrc</p> <p><code>xmodmap -e \"keycode 64 = Alt_L\"</code></p> <p><code>xmodmap -e \"keycode 205 = Alt_L\"</code></p>"},{"location":"natmeg/server/troubleshooting/Connection-error---pop-op-asking-for-authentification-for-update/","title":"Connection error - pop-op asking for authentification for update","text":"<p>Problem: a pop-op prompt saying \"Authentification is required to set the network proxy used for downloading packages\" keeps appears but require sudo right to continue.</p> <p></p> <p>This is possibly the same/related error:</p> <p></p> <p>Cause: It seems to be a generic error message caused outdated driver somewhere. Update 2021-01-21: this should be fixed now (@mcvinding)</p> <p>Solution: Probably that the drivers need to be updated by someone with sudo access. Until then, this post tells how to ignore the pop-up message permanently: Launch a Terminal Console and type <code>gnome-session-properties</code> and then uncheck the PackageKit Update Applet. Then restart your VNC server.</p> <p>The solution is from https://unix.stackexchange.com/questions/242423/banish-a-popup-error-message.</p>"},{"location":"natmeg/server/troubleshooting/MNE-coregistration-crash/","title":"MNE coregistration crash","text":"<p>Problem: MNE coregistration <code>mne.gui.coregistration()</code> in Python crash the entire session.</p> <p>Cause: might be some incompatibility between 3D render function and video drive (or lack thereof on Compute).</p> <p>Solution: Run this snippet of code in the terminal before starting Spyder: <code>export MESA_GL_VERSION_OVERRIDE=3.3</code></p>","tags":["MNE-Python","troubleshooting"]},{"location":"natmeg/server/troubleshooting/MNE-errors-when-plotting/","title":"MNE errors when plotting","text":"<p>Problem: The various MNE functions for plotting does not work. Especially 3D graphics such as plotting source estimates on cortical surfaces. In Spyder it might even crash the entire Python session.</p> <p>Cause: might be some incompatibility between 3D render function and video drive (or lack thereof on Compute). Possible also inconsistencies in Python dependencies.</p> <p>Solution: run this snippet of code in the terminal before starting Spyder: export MESA_GL_VERSION_OVERRIDE=3.3. Also, make sure that you install MNE according to the install instructions..</p>","tags":["MNE-Python","troubleshooting"]},{"location":"natmeg/server/troubleshooting/PyQt-error/","title":"PyQt5.QtWebKitWidgets error in Spyder","text":"<p>Problem: Spyder cannot open and gives an error <code>ModuleNotFoundError: No module named 'PyQt5.QtWebKitWidgets'</code></p> <p>Cause: The problem seems to be due to inconsistencies in where the package PyQt is located in the Anaconda environments. Can be due to using <code>pip</code> to install into an Anaconda environment (maybe). Might also be due to outdated versions of the package <code>PyQt</code>or <code>PyQt5</code>.</p> <p>Solution: In lack of a better solution, I removed the entirety of my Anaconda environment and created it again from scratch. If anybody has a better or simpler solution please add it!</p> <p>In this example, I create the environment by reinstalling MNE from the source. In other cases where the environment is not defined elsewhere, you might want to backup the environment into a text file. Like this: <code>conda env export &gt; environment.yaml</code></p> <p>1) Remove old environment</p> <p><pre><code>conda env remove --name mne\n</code></pre> 2) Create new environment (from MNE) <pre><code>conda install --channel=conda-forge --name=base mamba\nmamba create --override-channels --channel=conda-forge --name=mne mne\n</code></pre> For more information on installing MNE, see here: https://mne.tools/stable/install/manual_install.html</p> <p>3 Reinstall Spyder <pre><code>mamba install spyder\n</code></pre></p>","tags":["Python","PyQt5","Spyder","troubleshooting"]},{"location":"natmeg/server/troubleshooting/R-trouble-shooting/","title":"R/Rstudio trouble shooting","text":"<p>Here are some know issues with R/Rstudio on Compute.</p>","tags":["R","Rstudio","troubleshooting"]},{"location":"natmeg/server/troubleshooting/R-trouble-shooting/#install-rrstudio","title":"Install R/Rstudio","text":"<p>See here.</p>","tags":["R","Rstudio","troubleshooting"]},{"location":"natmeg/server/troubleshooting/R-trouble-shooting/#r-stan-problems","title":"r-stan problems","text":"<p>Problem: issues installing rstan-dependencies</p> <p>Solution: Set static download of V8 library</p> <pre><code>Sys.setenv(DOWNLOAD_STATIC_LIBV8=1)\ninstall.packages(\"V8\")\n</code></pre>","tags":["R","Rstudio","troubleshooting"]},{"location":"natmeg/server/troubleshooting/R-trouble-shooting/#update-r","title":"Update R","text":"<p>When starting RStudio it informs you that it is an outdated version and you should get the latest version from the website. Just click ignore. To update R your need to do it through Anaconda:</p> <pre><code>conda update r-base\n</code></pre>","tags":["R","Rstudio","troubleshooting"]},{"location":"natmeg/server/troubleshooting/R-trouble-shooting/#error-messages-when-starting-rstudio","title":"Error messages when starting Rstudio","text":"<p>I get several error messages in the terminal when I start Rstudio. Those I ignore. Rstudio starts and it works ok.</p>","tags":["R","Rstudio","troubleshooting"]},{"location":"natmeg/server/troubleshooting/R-trouble-shooting/#install-packages","title":"Install packages","text":"<p>Some packages (like lme4) cannot be installed through RStudio. To fix this, I needed to first install CMake. Cmake can be installed though Anaconda, like this:</p> <p><pre><code>conda install -c anaconda cmake \n</code></pre> After this, it was possible to install lme4 through R as usual:</p> <pre><code>install.packages(\"lme4\")\n</code></pre>","tags":["R","Rstudio","troubleshooting"]},{"location":"natmeg/server/troubleshooting/SFTP-%28or-samba%29-not-working/","title":"SFTP (or samba) not working","text":"<p>This can happen when the bash console on storage returns a message that is too large for the protocol. To fix it connect to storage02 with ssh (<code>username@storage02.natmeg.se</code>), open the bashrc file with a text editor (e.g., <code>nano ~/.bashrc</code>), comment out any <code>echo</code> commands by adding a <code>#</code> in front of it and save. </p>","tags":["SFTP","samba","connection","storage"]},{"location":"natmeg/server/troubleshooting/Scrambled-MEG-sensor-locations/","title":"Scrambled MEG sensor locations","text":"<p>Problem: the layout of the sensors is wrong. When plotting the position, they appear scrambled.</p> <p>Plot the channel positions:</p> <p><pre><code>plot3(data.grad.chanpos(:,1), data.grad.chanpos(:,2), data.grad.chanpos(:,3), 'o')\n</code></pre> </p> <p>How it should look:</p> <p></p> <p>Also, you cannot plot the sensor layout with <code>ft_plot_sens(data.grad)</code>: <pre><code>Error using ft_plot_sens (line 448)\ncannot work with balanced gradiometer definition\n</code></pre> WARNING: If you have this error, it is potentially a big problem if you are doing source reconstruction, as the geometry of the sensors relative to the brain is wrong (see the figures above again!). This another reason, why you should always plot the alignment of headpoints, the headmodel, and sensors before doing source reconstruction.</p> <p>The problem comes from the reconstruction of the channels when removing ICA components (<code>ft_rejectcomponent</code>). </p>","tags":["FieldTrip","troubleshooting"]},{"location":"natmeg/server/troubleshooting/Scrambled-MEG-sensor-locations/#solution","title":"Solution","text":"<p>When using <code>ft_rejectcomponent</code> make sure that you specify input data and set the configuration to NOT update sensor info:</p> <pre><code>% Remove components\ncfg = [];\ncfg.component   = reject_comp;\ncfg.channel     = 'MEG';\ncfg.updatesens  = 'no';\nicaclean_data = ft_rejectcomponent(cfg, comp, data);\n</code></pre>","tags":["FieldTrip","troubleshooting"]},{"location":"natmeg/server/troubleshooting/Terminal-does-not-open/","title":"Terminal does not open","text":"<p>In your local putty (Win) or Terminal (MacOS, Linux) once connected to the server:</p> <ol> <li> <p>Check if gnome-terminal is in /usr/bin: <code>which gnome-terminal</code></p> </li> <li> <p>Check version of gnome-terminal: <code>rpm -q gnome-terminal</code></p> </li> <li> <p>Verify gnome-terminal installation <code>rpm -V gnome-terminal</code></p> </li> <li> <p>If you are getting an error message that the environment variable LANG is not set, you may have mixed settings in the Language and Region section (for example, English and Sweden) in your local System Preferences. Try matching them (for example, English and United States).  </p> </li> <li> <p>Update XQuartz on your Mac</p> </li> <li> <p>Reset local environment variable 'export LIBGL_ALWAYS_INDIRECT=1'</p> </li> </ol> <p>If any error messages contact NatMEG core team.</p> <p>Disclaimer: The steps above worked for one user, but not 100% sure this was the solution.</p>","tags":["Terminal","troubleshooting"]},{"location":"natmeg/server/troubleshooting/Triggers/","title":"Errors with trigger channel STI016 in Neuromag data","text":"","tags":["FieldTrip","MNE-Python","troubleshooting"]},{"location":"natmeg/server/troubleshooting/Triggers/#errors-with-trigger-channel-sti016-in-neuromag-data","title":"Errors with trigger channel STI016 in Neuromag data","text":"<p>Problem: Triggers in the main trigger channel (STI101) are wrong. Some have negative values.</p> <p>There is a know issue when using TTL triggers with bit 16 during recordings of Neuromag MEG data. For some reason  STI016 in the Neuromag acquisition setups is stored with a negative value -32768 and subsequently subtracted rather than added to the total trigger value when read offline.</p> <p>Note that this problem does not appear in the Acquisition software or databrowser when recording data. The problem first appears when the data is written to disk and then loaded with MNE Python or FieldTrip for further off-line processing.</p> <p>Solution(s):</p> FieldTripMNE-Python <p>If you use FieldTrip, download the custom trial function ft_trialfun_neuromagSTI016fix from NatMEG's Github page (link). Copy/paste it into your FieldTrip directory in the folder ./fieldtrip/trialfun. Then when you read trials into FieldTrip, you must specify to use this trial function when reading data with ft_define_trial and ft_preprocessing.  The trial function will reconstruct STI101 from the split trigger channels and trigger values should be correct and you can proceed with your analysis as normal. For example, like this:</p> <p># Read data with reconstructed STI101 triggers <pre><code>cfg = [];\ncfg.dataset             = fname;\ncfg.trialdef.prestim    = 1;            # Time before trigger\ncfg.trialdef.poststim   = 1;            # Time after trigger\ncfg.trialdef.eventtype  = 'STI101';\ncfg.trialfun            = 'ft_trialfun_neuromagSTI016fix';\ncfg                     = ft_definetrial(cfg);\ndata                    = ft_preprocessing(cfg);\n</code></pre></p> <p>If you use MNE Python, this can be fixed with adding the argument unit_cast=True in mne.read_event. Like this: <code>eve = mne.find_events(mydata, unit_cast=True).</code></p> <p>Then you should get the correct trigger values in STI101 and can proceed as normal.</p>","tags":["FieldTrip","MNE-Python","troubleshooting"]},{"location":"natmeg/server/troubleshooting/Undefined-function-%27fiff_read_epochs%27/","title":"Undefined function 'fiff_read_epochs'","text":"<p>Problem: FieldTrip will not read raw fif files. Gives an error like this: <pre><code>Undefined function 'fiff_read_epochs' for input arguments of type 'char'.\n\nError in ft_read_header (line 1911)\n        epochs = fiff_read_epochs(filename);\n\nError in ft_read_header (line 134)\n    hdr{i} = ft_read_header(filename{i}, varargin{:});\n</code></pre> Cause: FieldTrip uses low-level MNE-Matlab function to read fif data files. As a default, these are not in any folder that FieldTrip adds to your path when running <code>ft_defaults</code>.</p> <p>Solution: Add the MNE functions to your path. Add the following line of code to your scripts at the stage you would call <code>ft_defaults</code>. <pre><code>addpath('~/fieldtrip/fieldtrip/external/mne')\n</code></pre> Assuming that you have FieldTrip in the folder <code>/home/your_username/fieldtrip/fieldtrip</code>; if not, change to wherever you have FieldTrip.</p>","tags":["FieldTrip","MNE","fif","read","epochs"]},{"location":"natmeg/server/troubleshooting/VNC-black-screen-error/","title":"VNC black screen error","text":"<p>You might experience a black screen with an error message when you first log in to a new VNC server.</p> <p></p> <p></p> <p>The error seems to be due to Anaconda/Python initiation, which messes with the VNC config in some way. I have not been able to solve the problem definitly [if you know, please tell me! @mcvinding], but the following procedure circumvents the problem:</p> <p>Open PuTTy to connect to Compute as usual. Then in terminal edit .bash_profile (the file that is executed when you log in with SSH) so it does not call .bashrc (the file that is executed every time you open a new terminal window).</p> <p>To edit the file: 1. Type <code>vim .bash_profile</code>.  2. Type <code>i</code> to enter edit mode. 3. Replace the lines:</p> <p><pre><code>if [ -f ~/.bashrc ]; then\n        . ~/.bashrc\nfi\n</code></pre> with </p> <p><pre><code>if [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n</code></pre> 4. Press <code>Esc</code> to exit edit mode and then type <code>:wq!</code> to exit.</p> <p>You should now be able to open the VNC viewer. If not, try to kill the VNC server (<code>vncserver -kill :X</code> where <code>X</code> is your VNC number) and start a new vncserver.</p> <p>With this fix Anaconda will only be started when you open a new terminal window in your VNC session. You do not need to modify .bashrc any more.</p>","tags":["troubleshooting"]},{"location":"natmeg/server/troubleshooting/rstanarm-install-problem/","title":"rstanarm install problem","text":"<p>If running into dependency troubles trying to install rstanarm-package</p> <p>This works if V8 fails to install.</p>","tags":["R","rstanarm","troubleshooting"]},{"location":"natmeg/server/troubleshooting/rstanarm-install-problem/#for-linux-download-libv8-during-installation","title":"For Linux: download libv8 during installation","text":"<pre><code>Sys.setenv(DOWNLOAD_STATIC_LIBV8=1)\ninstall.packages(\"V8\")\n</code></pre>","tags":["R","rstanarm","troubleshooting"]},{"location":"natmeg/squid-acquisition/01_TRIUX-specs/","title":"About","text":""},{"location":"natmeg/squid-acquisition/01_TRIUX-specs/#magnetoencephalography-meg","title":"Magnetoencephalography (MEG)","text":"<p>The core system at NatMEG is an  Elekta Neuromag TRIUX  306-channel system (102 magnetometers, 204 planar gradiometers).</p> <ul> <li>Further technical specifications are found here.</li> <li>System technical manual is found here.</li> </ul>"},{"location":"natmeg/squid-acquisition/02_Acquisition/","title":"Acquisition","text":"<p>Acquisition is the main program you need for running the MEG-recording.</p>"},{"location":"natmeg/squid-acquisition/02_Acquisition/#before-measurement","title":"Before measurement","text":"<p>Open: Menu -&gt; Neuromag -&gt; Acquisition</p> <p>! Always check for error messages in the top and check that gantry position is automatically detected. If not, se How to restart acquisition</p> <ol> <li>Load project</li> <li>Load settings <p>If you use STI channels it is recommended that you add all STI channels, even if you only use some of them. This is because of how the composite channels (STI101, STI102) is configured.</p> </li> <li>Add participant (as Patient)</li> </ol> <p>Do Digitisation (save preparation)</p> <ol> <li>Load preparation</li> <li>Always load the settings again, after loading preparation. If the same settings are not applied on the digitisation computer settings are unloaded.</li> </ol>"},{"location":"natmeg/squid-acquisition/02_Acquisition/#during-measurement","title":"During measurement","text":"<p>Use acquisition to handle the recording</p> <ol> <li>Press  GO!  to start recording buffer</li> </ol> <p>! Don't forget to check channels</p> <ol> <li>Check  cHPI  to record continuous head position</li> <li>Check  Record raw  to record raw file(s)</li> <li>Check  Average  to record average evoked file(s)</li> </ol>"},{"location":"natmeg/squid-acquisition/02_Acquisition/#after-measurement","title":"After measurement","text":"<ol> <li>Save data files</li> </ol> <p>! If Average box was checked, the first file to save will be the average file, then the raw file<p> ! Make sure to have a structured way of naming the files</p>"},{"location":"natmeg/squid-acquisition/02_Acquisition/#issues","title":"Issues","text":""},{"location":"natmeg/squid-acquisition/02_Acquisition/#how-to-restart-acquisition","title":"How to restart Acquisition?","text":"<p>Problem: Channels are not appearing when running Acquisition. Acquisition is giving errors about \"lost connection\" or \"cannot connect to channels\".</p> <p>Solution: In order do the following, if your problem keeps appearing then proceed to the next step; otherwise do not proceed:</p> <ol> <li>Check that the correct setting is loaded (File -&gt; Load Settings). See if the missing channels are still missing.</li> <li>Close and re-open Acquisition Programs (remember to save preparations if you have already begun).</li> <li>Restart Acquisition Programs. You find this option under the Neuromag top menu, \"Maintenance\". (Menu -&gt; Neruomag -&gt; Maintenance -&gt; Restart  Acquisition). A terminal will pop up\u2014type y to confirm. The restart might take a couple of minutes. Once the restart has completed, you need to restart Acquisition and also launch the Tuner and reload the current tunings you are using.</li> <li>If none of the above works, you will need to do a \"hard reset\". Open the text-file \"hard reset\" and follow the instructions. Wait a few minutes and then restart Acquisition according to 3.</li> </ol>"},{"location":"natmeg/squid-acquisition/03_Tuning/","title":"Tuning","text":"<p>Tuning is not always needed, but recommend at least for the first measure of the day.</p>"},{"location":"natmeg/squid-acquisition/03_Tuning/#before-measurement","title":"Before measurement","text":"<ol> <li>Open Acquisition</li> <li>Click the menu Tools -&gt; Tuner. The tuning tool will appear.</li> <li>Click file -&gt; load tuning. A message will pop up asking if you want to read the default state tuning. Click ok.</li> </ol> <p>! Optionally, you can click \"measure noise\" before loading the tuning and again after loading the tuning to see the noise level. The average noise level should be around 2.5-2.7.</p> <ol> <li>When at a satisfactory noise level, stop and save tuning (overwrite default tuning).</li> <li>Exit the Tuner (has to be done via the File menu)</li> </ol>"},{"location":"natmeg/squid-acquisition/03_Tuning/#during-measurement","title":"During measurement","text":"<p>No action required</p>"},{"location":"natmeg/squid-acquisition/03_Tuning/#after-measurement","title":"After measurement","text":"<p>No action required</p>"},{"location":"natmeg/squid-acquisition/03_Tuning/#issues","title":"Issues","text":""},{"location":"natmeg/squid-acquisition/03_Tuning/#the-average-noise-level-is-too-high-3-after-loading-the-default-tuning","title":"The average noise level is too high (&gt; 3) after loading the default tuning","text":"<p>Solution: Do the following:     1. Check that there are no objects in the MSR that could be causing disturbances, e.g. non-tested metallic stimulus equipment, left items, etc. Remove those items     2. Run new tuning: click \"measure noise\" and when it has measured the noise level, click \"Tune\". The tuning procedure with iterate through tuning parameters and try to minimize the noise in the system. Each iteration takes about 20 seconds. Click \"stop tuning\" when the average noise level is below 2.7.</p> <p>! The tuning procedure takes up to 15 min. Make sure that you have enough time to run the tuning procedure and always check the tuning well in advance before your participant arrives.</p> <p>! If sensors are missing, they are usually at a too high noise level. Ctrl-click on very noisy (or missing) sensors and heat via the menu. If many sensors are missing you can heat all sensors (this takes a couple of minutes). If problem remains, you might have to restart Acquisition (see this Acquisition section) before continuing with the tuning or call for assistance.</p>"},{"location":"natmeg/squid-acquisition/04_Check-channels/","title":"Check channels","text":"<p>After tuning and before each measurement you should check the channels.</p>"},{"location":"natmeg/squid-acquisition/04_Check-channels/#before-measurement","title":"Before measurement","text":"<ol> <li>Click  GO!  in the Acquisition control window</li> <li>Browse through channels to see if everything looks alright</li> <li>If everything looks good you are ready to start recording, if not see below</li> </ol>"},{"location":"natmeg/squid-acquisition/04_Check-channels/#during-measurement","title":"During measurement","text":"<p>If door is opened or between condition when recording has been stopped redo the steps above</p>"},{"location":"natmeg/squid-acquisition/04_Check-channels/#after-measurement","title":"After measurement","text":"<p>No action required</p>"},{"location":"natmeg/squid-acquisition/04_Check-channels/#issues","title":"Issues","text":""},{"location":"natmeg/squid-acquisition/04_Check-channels/#fixing-bad-channels-before-recording","title":"Fixing bad channels before recording","text":"<p>Problem: Jumpy or noisy channels</p> <p>Solution: Use Squiddler to heat bad channels</p> <p>Open: Menu -&gt; Neuromag -&gt; Squiddler</p> <p></p> <p>In Squiddler:</p> <ol> <li>Select channel with slider</li> <li>Open Commands, click Heat Channel. Wait until the channels settle then inspect if the channel looks fine. Inspect if other channels have been affected by the heating.</li> <li>Repeat 1-2 for all bad channels.</li> </ol>"},{"location":"natmeg/squid-acquisition/05_EEG-specs/","title":"Electroencephalography (EEG).","text":""},{"location":"natmeg/squid-acquisition/05_EEG-specs/#procedure","title":"Procedure","text":"<p>This is preferably a two-person job (running time, approximation).</p> <ol> <li>Try to ask participants for their head-size so you can prepare a cap in advance.</li> <li>Fit a suitable cap</li> <li> <p>Measure length from nasion to back-head-bump</p> <p>Forehead sensors should be at 10% of the length</p> </li> <li> <p>Tighten the cap (time: 2 min)</p> </li> <li> <p>Fit the EOG and ECG (EMG) electrodes to the usual spots (to the best of your ability). Some electrodes might be in the way of the EEG-electrodes but that is fine. (time: 6 min)</p> <p>Make sure the settings include the EEG-channels that you want. Otherwise, the digitization program will not recognize them.</p> </li> <li> <p>Digitize as usual. You can put the coil on the cap if there is no room. (time: 20 min)</p> </li> <li> <p>Pen procedure (with EEG) (time: 40-60 min)</p> </li> <li> <p>Connect the EEG adapter to the impedance meter.</p> </li> <li>Connect reference and ground to the adapter, there are separate cables available for this.</li> <li> <p>Connect the reference and ground.</p> </li> <li> <p>Fill the EEG-electrodes with gel (person 1) while simultaneously checking the impedance (person 2). Add more gel if needed. You can also use an EEG-needle and carefully scratch the head.</p> </li> </ol>"},{"location":"natmeg/squid-acquisition/05_EEG-specs/#specifications","title":"Specifications","text":"<p>The Elekta Neuromag TRIUX  has an integrated MEG-compatible  128 channel EEG system.</p> <ul> <li>Further technical specifications are found here.</li> <li>System technical manual is found here.</li> </ul> <p>We currently use 128 channel caps, 74 channel caps and 128-slit custom montage caps from  EasyCap</p>"},{"location":"natmeg/squid-acquisition/06_Biochannels-specs/","title":"Biochannels","text":"<p>The Elekta Neuromag TRIUX   also has 12 integrated bipolar channels, for recording of electrooculgraphy (EOG), electrocardiography (ECG), and electromyography (EMG).</p> <ul> <li>Further technical specifications are found here.</li> <li>System technical manual is found here.</li> </ul> <p>We currently use the EL508 electrodes and  LEAD108 and LEAD108a from BioPac for most biochannel measurements. We also use  the EL254RT  for facial EMG.</p>"},{"location":"natmeg/squid-acquisition/07_Lab-notebook/","title":"Lab notebook","text":"<p>The Lab notebook helps you to note important things for your project. You can add project specific details in the Project column, subject specific details in the Subject column or measurement specific details in the Date column. The Lab notebook is saved as json-files and txt-files in your project folder.</p>"},{"location":"natmeg/squid-acquisition/07_Lab-notebook/#before-measurement","title":"Before measurement","text":"<ol> <li>Start a notebook server from the desktop (if one is not already running in the Terminal)</li> <li>Start Lab notebook from the desktop</li> <li>Choose Project, Subjects and Date (automatically todays date)</li> </ol>"},{"location":"natmeg/squid-acquisition/07_Lab-notebook/#during-measurement","title":"During measurement","text":"<ol> <li>Note everything of interest for the experiment, preferably in the Date column</li> <li>One thing to note may be subject's sleepiness during the measurement according to the Karolinska sleepiness scale (KSS) which should be on the desk or on the Stimulus computer's Desktop.</li> </ol>"},{"location":"natmeg/squid-acquisition/07_Lab-notebook/#after-measurement","title":"After measurement","text":"<p>Make sure all notes have been added and close the Lab notebook</p>"},{"location":"natmeg/squid-acquisition/07_Lab-notebook/#issues","title":"Issues","text":""},{"location":"natmeg/squid-acquisition/07_Lab-notebook/#lab-notebook-does-not-start","title":"Lab Notebook does not start","text":"<p>Problem: If you cannot open Lab Notebook and get and error message saying Firefox is already running. Solution: Open a new terminal and type:</p> <p>pkill -f firefox</p> <p>This shuts down all Firefox processes. Then open the notebook server and the notebook from the desktop.</p>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/","title":"Audio mixer","text":"<p>See figure below for an overview of how to control the sound mixer. The mixer should as default always be on. If it is switched off, use the on/off switch on the backside of the mixer to switch it back on.</p> <p> Figure 1. Mackie Big Knob Studio+ with labels</p> <p>A. Select input. Sound from  Audiofile (3/4) and/or Stimulus Computer (5/6)</p> <p>B. Select the MSR backpanal loudspeakers as output and adjust the volume.</p> <p>C. Select the Control Room speakers (the small speakers next to the mixer) as output. Soundtubes always recieve sound.</p> <p>D. Talkback. Hold down either button to use the mixer to speak to the participant inside the MSR.    - Left button: speak though sound tubes.    - Right button: speak though the backpanel speakers.</p> <p>E. Microphone for talkback.</p> <p>F. Main volume adjustment</p>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#before-measurement","title":"Before measurement","text":"<ol> <li>Make sure speakers in the control room are turned on.</li> </ol> <p>! There are separate speakers for the stimuli audio and the talkback microphone. The left ones are for stimuli audio and the right ones are for talkback.</p> <ol> <li>Make sure audio mixer is on and that you have the right configuration (MSR speakers or sound tubes)</li> </ol> <p>! If using the sound tubes be carful not to have too loud talkback volume</p> <ol> <li>Check volume with participant</li> </ol>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#during-measurement","title":"During measurement","text":"<p>Remember to press the correct talkback button when communicating the the participant (MSR speakers or sound tubes)</p>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#after-measurement","title":"After measurement","text":"<ol> <li>Turn off speakers. Do not turn off audio mixer.</li> <li>If using sound tubes make sure to through away eartips and wipe tubes with alcohol</li> </ol>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#specifications","title":"Specifications","text":""},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#audio-mixer-mackie-big-knob-studio","title":"Audio mixer (Mackie Big Knob Studio+)","text":"<p>The Audio mixer receives input from the Stimulus PC, AudioFile and the Motu interface. It outputs to the MSR speakers and the control room speakers. The built-in microphone is used for communication with a person inside the MSR either via the MSR speakers or via the ear-tubes.</p> <p> Figure 2. Mackie Big Knob Studio+ front</p> <p> Figure 3. Mackie Big Knob Studio+ back</p> <ul> <li>Further technical specifications are found here.</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#issues","title":"Issues","text":""},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#there-is-no-sound-to-the-msr","title":"There is no sound to the MSR","text":"<p>Problem: sound from the Stimulation Computer, Audiofile, or talkback does not appear inside the MSR</p> <p>Solution: Do the following</p> <ul> <li>Check that the sound on the Stimulation Computer is on and at full volume.</li> <li>Make sure that the correct input is selected (A in the figure above).</li> <li>Make sure that the correct output is selected (B or C in the figure above).</li> <li>For sound tubes: check that the sound tube amplifier is switched on. The sound tube amplifier is located in the Stimulus Cabinet. Switch it on if it is switched off. WARNING: do not switch it on a participant has the sound tubes in their ears.</li> <li>For backpanel speakers: check that the backpanel speaker amplifiers are switched on. The amplifiers are located on the top shelf of the MSR Cabinet. Switch on the amplifiers (the two small boxed; they should be blinking rapidly) and the input/output board if any are switched off.</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#there-is-no-sound-from-the-msri-cannot-hear-the-participant","title":"There is no sound from the MSR/I cannot hear the participant","text":"<p>Problem: there is no sound from the MSR, and I cannot hear if the participant is saying anything</p> <p>! Your participant should, at all times, be able to speak to you. As the first thing, after you have positioned the participant in the scanner and left the MSR, you must test that the participant can hear you and you can hear them.</p> <p>Solution: Do the following</p> <ul> <li>Make sure that the MSR monitor (the big loudspeaker on top of the Stimulation Cabinet) is switched on. The on/off switch is on the backside of the MSR monitor. The light in the logo on the front of the monitor will be on when the monitor is turned on.</li> <li>Make sure that you are using the correct talkback option; i.e. using the sound tube talkback when using the sound tubes.</li> <li>Make sure that the sound mixer is turned on. The sound also go through the video mixer; make sure that the video mixer is also turned on. Buttons on the video mixer have a red light when they are turned on. There is a small on/off switch on the back of the video mixer to turn it back on if it has been turned off.</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Audio-mixer/#nothing-is-working-is-there-another-way","title":"Nothing is working, is there another way?","text":"<p>Yes, you can use the built in Triux system intercom. Do the following.</p> <p></p> <p>! If the microphone is not plugged in find it and plug it in on the side of the MEG.</p> <ol> <li>Dial 11 on the intercom. A two-way communication is no open.</li> <li>Press and hold T (for a few seconds) to turn off the microphone in the control room. The MSR microphone should still be on.</li> <li>To temporarily open the microphone hold T and speak. Alternatively, to turn on the microphone permanently press T briefly.</li> <li>To close the intercom press X.</li> </ol>"},{"location":"natmeg/stimulus-equipment/audio/Audiofile-specs/","title":"AudioFile","text":"<p>The audio stimulation is typically controlled via the AudioFile stimulator from Cambridge Research Systems Limited, in which case  the sound onset jitter is below 1 ms.</p> <p>Further technical specifications for the AudioFile are found here.</p>"},{"location":"natmeg/stimulus-equipment/audio/B%26K-sound-level-meter-specs/","title":"B&K sound level meter and artificial ear","text":""},{"location":"natmeg/stimulus-equipment/audio/B%26K-sound-level-meter-specs/#bruel-kjaer-sound-level-meter-2235","title":"Br\u00fcel &amp; Kjaer sound level meter 2235","text":"<p>S/N: 1853691. Includes preamp ZC0020  </p> <p>NB: Take note of Frequency and Time Weighting settings when using.</p> <p></p>"},{"location":"natmeg/stimulus-equipment/audio/B%26K-sound-level-meter-specs/#bruel-kjaer-artificial-ear-4152","title":"Br\u00fcel &amp; Kjaer Artificial Ear 4152","text":"<p>S/N: 1832705  </p> <p>Use coupled to sound level meter to measure sound level in soundtubes. Not suited for measuring levels in free field (i.e from speakers).</p> <p></p>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/","title":"Behringer specs","text":""},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#berhinger-ultragain-digital-ada8200","title":"Berhinger ULTRAGAIN DIGITAL ADA8200","text":""},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#ultragain-digital-ada8200","title":"ULTRAGAIN DIGITAL ADA8200","text":"<p>S/N: -</p> <p>There are two ULTRAGAIN DIGITAL ADA8200 in the lab, one in the Stimulus cabinet (Unit A) and one in the MSR cabinet (Unit B). They are connected to each other via an 8-channel ADAT link.</p> <p> </p> <p>Unit B receives input from the microphone via the Amplifier, sends the signal to Unit A which outputs it to the Speaker.</p> <p>Unit A receives input from the Audio interface and sends the signal to unit B which outputs it to the MSR speakers amplifier.</p> <p>Some notes of caution:  - The ADAT units should never be touched by someone other than the NatMEG core team.  - Phantom power should always be turned off (=green light).  - Since the ADAT units are generally always on they tend to only last for a few years. No or cracking sound together with irregular light flashes generally means it is time to replace the unit.</p>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#full-description","title":"Full Description","text":""},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#adat-digital-connectivity","title":"ADAT Digital Connectivity","text":"<p>Featuring innovative Cirrus Logic 24-bit A/D \u2013 D/A converters the ADA2800 can be used to achieve incredible audio/digital conversion with a detailed and pristine tone. The ADAT inputs and outputs can operated simultaneously or independently with a word clock and operate at either 44.1 or 48 kHz conversion rates both with full 24-bit resolution.</p>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#legendary-midas-preamps","title":"Legendary MIDAS Preamps","text":"<p>Loaded with 8 premium MIDAS designed microphone preamps the ADA8200 provides dynamic and crystal clear audio reproduction perfect for commercial quality recordings. Since its founding in the 1970s MIDAS has become one of the world\u2019s leading manufacturers of audio mixing consoles and equipment due to their no compromise philosophy and meticulous design process.</p>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#features","title":"Features","text":"<ul> <li>Professional microphone preamp and A/D converter</li> <li>8 x MIDAS designed microphone preamps</li> <li>Reference class cirrus logic 24-Bit converters for excellent A/D \u2013 D/A conversion</li> <li>Premium XLR and 1/4\" TRS connectivity</li> <li>+48 Switchable phantom power on all channels</li> <li>Can be used as a master clock, sync'd to ADAT or word</li> <li>High fidelity 44.1 / 48 kHz sample rate</li> <li>1U Rackmountable Design</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#specifications","title":"Specifications","text":""},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#general","title":"General","text":"<ul> <li>Channels: 8</li> <li>A to D: Yes</li> <li>D to A: Yes</li> <li>Sample Rate: 44.1/48 kHz</li> <li>Bit Depth: 24-Bit</li> <li>Analog Inputs: 8 x XLR, 8 x TRS</li> <li>Analog Outputs: 8 x XLR</li> <li>Digital Inputs: 1 x ADAT</li> <li>Digital Outputs: 1 x ADAT</li> <li>Clock Inputs: 1 x Word Clock</li> <li>Rack Spaces: 1U</li> <li>Height: 1.75\u201d</li> <li>Depth: 8.5\u201d</li> <li>Width: 19\u201d</li> <li>Weight: 2.08 Kg</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#microphone-inputs","title":"Microphone Inputs","text":"<ul> <li>Design: MIDAS Designed Microphone Preamp</li> <li>Type: XLR</li> <li>Gain Range: +15 to + 60dB</li> <li>Input Level (Max): +6 dBu @ +10 dB Gain</li> <li>Impedance (Balanced): 2.7 k Ohms</li> <li>Phantom Power: +48 V Switchable</li> <li>Line Inputs</li> <li>Type: 1/4\" TRS</li> <li>Impedance: 20 k Ohms (Balanced), 10 k Ohms (Unbalanced)</li> <li>Gain Range: -5 to +40 dB</li> <li>Input Level (Max): +16 dBu @ 0 dB FS</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#line-outputs","title":"Line Outputs","text":"<ul> <li>Type: XLR</li> <li>Impedance: 1 k Ohms (Balanced), 500 Ohms (Unbalanced)</li> <li>Output Level (Max): +16 dBu @ 0 dB FS</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#digital-inputoutput","title":"Digital Input/Output","text":"<ul> <li>Type: TOSLINK Optical Connector</li> <li>Format: ADAT, 8 Channels, 24-Bit @ 44.1 / 48 kHz</li> <li>Synchronization Source: Internal 44.1 / 48 kHz, ADAT Input, Word Clock Input</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#word-clock-input","title":"Word Clock Input","text":"<ul> <li>Type: BNC Connector</li> <li>Input Level: 2 to 6 V (Peak-to-Peak)</li> <li>Frequency Range: 44.1 to 48 kHz</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Behringer-specs/#system-specifications","title":"System Specifications","text":"<ul> <li>Frequency Range: 10 Hz \u2013 24 kHz @ 48 kHz Sample Rate</li> <li>THD: 0.008</li> <li>Crosstalk: -87 dB</li> </ul> <p>Quick Start guide (download pdf)</p>"},{"location":"natmeg/stimulus-equipment/audio/Soundtubes-specs/","title":"Soundtubes specs","text":""},{"location":"natmeg/stimulus-equipment/audio/Soundtubes-specs/#soundtubes","title":"SoundTubes","text":"<p>The headphones are binaural SoundTubes, model ADU1c, from KAR Audio. The stimulator-to-ear delay is about 10 msec.</p>"},{"location":"natmeg/stimulus-equipment/audio/Soundtubes-specs/#soundtube-speaker","title":"SoundTube speaker","text":"<p>For use with custom (3.3m) sound tubes and in-ear foam tips.</p> <p></p> <p>Frequency response according to manual:</p> <p></p> <ul> <li>Further technical specifications are found here.</li> </ul>"},{"location":"natmeg/stimulus-equipment/audio/Speakers-specs/","title":"Speakers specs","text":""},{"location":"natmeg/stimulus-equipment/audio/Speakers-specs/#speakers","title":"Speakers","text":"<p>The  speakers are  Sound Showers, model SSHP60X60W, from PanPhonics. The stimulator-to-ear delay is about 10 msec. Sound to the speakers is controlled by the Audio-mixer.</p> <ul> <li>Further technical specifications are found here.</li> </ul>"},{"location":"natmeg/stimulus-equipment/other/ENS-specs/","title":"ENS specs","text":""},{"location":"natmeg/stimulus-equipment/other/ENS-specs/#electric-nerve-stimulation","title":"Electric Nerve Stimulation","text":"<p>For  electrical nerve stimulation, we use two DeMeTec SCG30 stimulators, either with felt tips for median nerve stimulation, or with ring electrodes for single phalange stimulation.</p> <p>The trigger to stimulation delay is about 3 msec.</p> <ul> <li>Further technical specifications are found here.</li> </ul>"},{"location":"natmeg/stimulus-equipment/other/Mechanical-pain-specs/","title":"Mechanical pain specs","text":""},{"location":"natmeg/stimulus-equipment/other/Mechanical-pain-specs/#mechanical-pain-stimulation","title":"Mechanical pain stimulation","text":"<p>For mechanical pain stimulation, we use a custom-built pressure algometer from Somedic.</p> <p>The trigger to stimulation delay is under evaluation.</p> <ul> <li>Further technical specifications are found here.</li> </ul>"},{"location":"natmeg/stimulus-equipment/other/Olfaction-specs/","title":"Olfaction specs","text":""},{"location":"natmeg/stimulus-equipment/other/Olfaction-specs/#olfactory-stimulation","title":"Olfactory Stimulation","text":"<p>For  olfactory stimulation, we use a  Monell 9-channel olfactometer. The trigger to stimulation delay is under evaluation. Further technical specifications  TBA</p> <ul> <li>Further technical specifications are found TBA.</li> </ul>"},{"location":"natmeg/stimulus-equipment/other/Pleasant-touch-specs/","title":"Pleasant touch specs","text":""},{"location":"natmeg/stimulus-equipment/other/Pleasant-touch-specs/#pleasant-touch-stimulation","title":"Pleasant touch stimulation","text":"<p>For pleasant touch stimulation, we use  custom-built 2-arm brush robot system from NatMEG Guest Professor Veikko Jousm\u00e4ki.</p> <p>The trigger to stimulation delay is completely dependent on brush robot sequences. A combination of brush arm speedometer, accelerometer, load cell and brush touch onset optic sensors are used to assess the movements and stimulation periods.</p> <ul> <li>Further technical specifications TBA</li> </ul>"},{"location":"natmeg/stimulus-equipment/other/Tactile-specs/","title":"Tactile specs","text":""},{"location":"natmeg/stimulus-equipment/other/Tactile-specs/#tactile-stimulation","title":"Tactile stimulation","text":"<p>For tactile fingertip, toe and lip stimulation, we use a 4-channel Somatosensory Stimulus System from Biomagnetic Technologies.</p> <p>The trigger to stimulation delay is under evaluation.</p> <p>Further technical specifications are found here.</p>"},{"location":"natmeg/stimulus-equipment/visual/Screen-and-projector/","title":"Screen and projector","text":""},{"location":"natmeg/stimulus-equipment/visual/Screen-and-projector/#equipment-required","title":"Equipment required","text":"<ul> <li>The screen</li> </ul>"},{"location":"natmeg/stimulus-equipment/visual/Screen-and-projector/#before-measurement","title":"Before measurement","text":"<ol> <li>Drag the mirror into the right position</li> <li>Place monitor in front of the mirror at the markings on the floor</li> <li>Turn on projector by pressing on time at the remote control</li> </ol> <p>! If participant is seated in the chair, be careful not to blind the participant with the light from the projector</p>"},{"location":"natmeg/stimulus-equipment/visual/Screen-and-projector/#during-measurement","title":"During measurement","text":"<p>Just remember that the participant can see what you see on the screen.</p>"},{"location":"natmeg/stimulus-equipment/visual/Screen-and-projector/#after-measurement","title":"After measurement","text":"<ol> <li>Move the screen away from the chair</li> <li>Turn off the projector by pressing twice on the remote</li> </ol>"},{"location":"natmeg/stimulus-equipment/visual/Screen-and-projector/#projector-specifications","title":"Projector specifications","text":"<p>For  visual stimulation, we use  a   FL35 LED  DLP projector from Projection Design.  The current installation runs at 32 bit color in 1920*1080  @  120 fps..  Maximum  field of view for visual presentation is about  63 (horizontal) * 41 (vertical) degrees.</p> <p>In standard settings (16:9 ratio, 1920x1080 resolution, 120 fps, 100 cm distance), the projector fills a 72 x 44 cm rectangle on the screen, ca 40 x 25 degrees of the visual field.</p> <p>Presenting a white picture on the screen gives 100 lumen on screen, 30 lumen at eye position.</p> <p>The visual stimulation is typically controlled via Neurobs Presentation, in which case  the computer-to-eye delay is about 8 ms, with no frame drops or frame doubles.</p> <ul> <li>Further technical specifications for the projector are found here.</li> </ul>"},{"location":"natmeg/stimulus-equipment/visual/Showing-visual-stimuli-on-the-screen/","title":"Showing visual stimuli on the screen","text":"","tags":["visual","screen","projector","stimulus"]},{"location":"natmeg/stimulus-equipment/visual/Showing-visual-stimuli-on-the-screen/#equipment-you-need","title":"Equipment you need:","text":"<ul> <li>The screen</li> </ul>","tags":["visual","screen","projector","stimulus"]},{"location":"natmeg/stimulus-equipment/visual/Showing-visual-stimuli-on-the-screen/#before-measurement","title":"Before measurement","text":"<ol> <li>Drag the mirror into the right position</li> <li>Place monitor in front of the mirror at the markings on the floor</li> <li>Turn on projector by pressing on time at the remote control     &gt; ! If participant is seated in the chair, be careful not to blind the participant with the light from the projector</li> </ol>","tags":["visual","screen","projector","stimulus"]},{"location":"natmeg/stimulus-equipment/visual/Showing-visual-stimuli-on-the-screen/#during-measurement","title":"During measurement","text":"<p>Just remember that the participant can see what you see on the screen.</p>","tags":["visual","screen","projector","stimulus"]},{"location":"natmeg/stimulus-equipment/visual/Showing-visual-stimuli-on-the-screen/#after-measurement","title":"After measurement","text":"<ol> <li>Move the screen away from the chair</li> <li>Turn off the projector by pressing twice on the remote</li> </ol>","tags":["visual","screen","projector","stimulus"]},{"location":"server/connect/Connect-to-server/","title":"Connect to server","text":"<p>TBA</p>","tags":["server","connect"]}]}